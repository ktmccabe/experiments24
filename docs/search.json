[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiments 2024",
    "section": "",
    "text": "Course Notes\nThis document will include important links and course notes for Experimental Methods in Political Science."
  },
  {
    "objectID": "index.html#getting-set-up-in-r",
    "href": "index.html#getting-set-up-in-r",
    "title": "Experiments 2024",
    "section": "Getting Set Up in R",
    "text": "Getting Set Up in R\nGoal\nBy the end of the first week of the course, you will want to have R and RStudio installed on your computer (both free), feel comfortable using R as a calculator, and making documents using the R Markdown file type within RStudio.\nR is an application that processes the R programming language. RStudio is also an application, which serves as a user interface that makes working in R easier. We will primarily open and use RStudio to work with R.\nIn other classes, you may come across Stata, SPSS, Excel, or SAS, which are programs that also conduct data analysis. R has the advantage of being free and open-source. Even after you leave the university setting, you will be able to use R/RStudio for free. As an open-source program, it is very flexible, and a community of active R/RStudio users is constantly adding to and improving the program. You might also encounter the Python language at some point. R and Python have similarities, and learning R can also make learning Python easier down the road.\nR and RStudio Installation\n\nThis video from Professor Christopher Bail explains why many social scientists use R and describes the R and RStudio installation process. This involves\n\nGoing to cran, select the link that matches your operating system, and then follow the installation instructions, and\nVisiting RStudio and follow the download and installation instructions. R is the statistical software and programming language used for analysis. RStudio provides a convenient user interface for running R code."
  },
  {
    "objectID": "index.html#open-r-script-in-rstudio",
    "href": "index.html#open-r-script-in-rstudio",
    "title": "Experiments 2024",
    "section": "Open R Script in RStudio",
    "text": "Open R Script in RStudio\nThis next section provides a few notes on using R and RStudio now that you have installed it. In this section, we cover the following materials:\n\nUsing R as a calculator and assigning objects using &lt;-\nSetting your working directory and the setwd() function.\nCreating and saving an R script (.R file)\nCreating, saving, and compiling an R Markdown document (.Rmd) into an html document (.html)\n\nRStudio is an open-source and free program that greatly facilitates the use of R, especially for users new to programming. Once you have downloaded and installed R and RStudio, to work in R, all you need to do now is open RStudio (it will open R). It should look like this, though your version numbers will be different:\n\nNote: The first time you open RStudio, you likely only have the three windows above. We will want to create a fourth window by opening an R script to create the fourth window.\n\nTo do this, in RStudio, click on File -&gt; New -&gt; R script in your computer’s toolbar. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment.\n\nYou can alternatively click on the green + sign indicator in the top-left corner of the RStudio window, which should give you the option to create a new R script document.\n\n\nNow you should have something that looks like this:\n\n\nThe upper-left window has our .R script document that will contain code.\nThe lower-left window is the console. This will show the output of the code we run. We will also be able to type directly in the console.\nThe upper-right window shows the environment (and other tabs, such as the history of commands). When we load and store data in RStudio, we will see a summary of that in the environment.\nThe lower-right window will enable us to view plots and search help files, among other things.\n\n\nUsing R as a Calculator\nThe bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard:\n\n5 + 3\n\n[1] 8\n\n5 - 3\n\n[1] 2\n\n5^2\n\n[1] 25\n\n5 * 3\n\n[1] 15\n\n5/3\n\n[1] 1.666667\n\n(5 + 3) * 2\n\n[1] 16\n\n\nAgain, in the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course.\n\n\nWorking in an R Script\nEarlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window.\nSet your working directory setwd()\nMany times you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets.\nThere are many ways to do this.\n\nAn easy way is to go to Session -&gt; Set Working Directory -&gt; Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it.\nNote: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future.\nIf you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now\n\n\n## Example of where my directory was\ngetwd()\n\n[1] \"/Users/ktmccabe/Dropbox/GitHub2/experiments24\"\n\n\nIf I want to change the working directory, I can go to the top toolbar of my computer and use Session -&gt; Set Working Directory -&gt; Choose Directory or just type my file pathway using the setwd() below:\n\n## Example of setting the working directory using setwd().\n## Your computer will have your own file path.\nsetwd(\"/Users/ktmccabe/Dropbox/Rutgers Teaching/\")\n\n\n\nSaving the R Script\nLet’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory.\nGive the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension.\n\n\nAnnotating your R script\nNow that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now.\n\nJust like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments– annotations that explain what your code is doing. Below is a screenshot example of a template R script.\nYou can specify your working directory at the top, too. Add your own filepath inside setwd()\n\n\n\nThen you can start answering problems in the rest of the script.\nThink of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script.\n\n\n\nRunning Commands in your R script\nThe last thing we will note in this section is how to execute commands in your R script.\nTo run / execute a command in your R script (the upper left window), you can\n\nHighlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows\nPlace your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or\nDo 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window.\n\nTry it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this:\n\nAfter you executed the code, you should see it pop out in your Console:\n\n5 + 3\n\n[1] 8\n\n\n\nNote: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do.\n\n## Example\nsum53 &lt;- 5 + 3 # example of assigning an addition calculation\n\n\n\nObjects\nSometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53:\n\nsum53 &lt;- 5 + 3\n\nAfter we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just has sum53, it will output 8. Try it:\n\nsum53\n\n[1] 8\n\n\nNow we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do:\n\nsum53 - 2\n\n[1] 6\n\n\nLet’s say we wanted to divide two stored calculations:\n\nten &lt;- 5 + 5\ntwo &lt;- 1 + 1\nten / two\n\n[1] 5\n\n\nThe information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks.\n\nmccabe &lt;- \"professor for this course\"\nmccabe\n\n[1] \"professor for this course\"\n\n\nNote: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.\n\nGOOD CODE: practice.calc &lt;- 5 + 3\nBAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3\n\nWhile these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course.\nWe can also store an array or “vector” of information using c()\n\nsomenumbers &lt;- c(3, 6, 8, 9)\nsomenumbers\n\n[1] 3 6 8 9\n\n\nImportance of Clean Code\nIdeally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused.\nFor example, R is case sensitive. Let’s say we assigned our object like before:\n\nsum53 &lt;- 5 + 3\n\nHowever, when we went to execute sum53, we accidentally typed Sum53:\n\nSum53\n\nError in eval(expr, envir, enclos): object 'Sum53' not found\n\n\nOnly certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2:\n\nmccabe / 2\n\nError in mccabe/2: non-numeric argument to binary operator\n\n\nA big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages."
  },
  {
    "objectID": "index.html#r-markdown",
    "href": "index.html#r-markdown",
    "title": "Experiments 2024",
    "section": "R Markdown",
    "text": "R Markdown\nAn R Markdown document, which you can also create in RStudio, allows you to weave together regular text, R code, and the output of R code in the same document. This can be very convenient when conducting data analysis because it allows you more space to explain what you are doing in each step. We will use it as an effective platform for writing up problem sets.\nR Markdown documents can be “compiled” into html, pdf, or docx documents by clicking the Knit button on top of the upper-left window. Below is an example of what a compiled html file looks like.\n\nNote that the image has both written text and a gray chunk, within which there is some R code, as well as the output of the R code (e.g., the number 8 and the image of the histogram plot. \n\nWe say this is a “compiled” RMarkdown document because it differs from the raw version of the file, which is a .Rmd file format. Below is an example of what the raw .Rmd version looks like, compared to the compiled html version.\n \n\nGetting started with RMarkdown\nJust like with a regular R script, to work in R Markdown, you will open up RStudio.\n\nFor additional support beyond the notes below, you can also follow the materials provided by RStudio for getting started with R Markdown https://rmarkdown.rstudio.com/lesson-1.html.\n\nThe first time you will be working in R Markdown, you will want to install two packages: rmarkdown and knitr. You can do this in the Console window in RStudio (remember the lower-left window!).\nType the following into the Console window and hit enter/return.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\nOnce you have those installed, now, each time you want to create an R Markdown document, you will open up a .Rmd R Markdown file and get to work.\n\nGo to File -&gt; New File -&gt; R Markdown in RStudio\n\nAlternatively, you can click the green + symbol at the top left of your RStudio window\n\nThis should open up a window with several options, similar to the image below\n\nCreate an informative title and change the author name to match your own\nFor now, we will keep the file type as html. In the future, you can create pdf or .doc documents. However, these require additional programs installed on your computer, which we will not cover in the course.\n\n\n\n\nAfter you hit “OK” a new .Rmd script file will open in your top-left window with some template language and code chunks, similar to the image below. Alternatively, you can start from scratch by clicking “Create Empty Document” or open a template .Rmd file of your own saved on your computer.\n\n\n\nSave as .Rmd file. Save the file by going to “File -&gt; Save as” in RStudio\n\n\nGive the file an informative name like your LastnamePractice1.Rmd\n\n\nKey Components. Now you are ready to work within the Rmd script file. We will point to four basic components of this file, and you can build your knowledge of RMarkdown from there.\n\nThe top part bracketed by --- on top and bottom is the YAML component. This tells RStudio the pertinent information about how to “compile” the Rmd file.\n\nMost of the time you can leave this alone, but you can always edit the title, author, or date as you wish.\n\nThe next component are the global options for the document. It is conveniently labeled “setup.” By default what this is saying is that the compiled version will “echo” (i.e., display all code chunks and output) unless you specifically specify otherwise. For example, note that it says include = FALSE for the setup chunk. That setting means that this code chunk will “run” but it will not appear in the nicely compiled .html file.\n\nMost of the time you will not need to edit those settings.\n\nThe third component I want to bring attention to is the body text. The # symbol in RMarkdown is used to indicate that you have a new section of the document. For example, in the compiled images at the beginning, this resulted in the text being larger and bolded when it said “Problem 2.” In addition to just using a single #, using ## or ### can indicate subsections or subsubsections. Other than that symbol, you can generally write text just as you would in any word processing program, with some exceptions, such as how to make text bold or italicized.\nThe final component I want to call attention to are the other main body code chunks. These are specific parts of the document where you want to create a mini R script. To create these, you can simply click the + C symbol toward the top of the top left window of RStudio and indicate you want an R chunk.\n\n\n\n\nWriting R Code. Within a code chunk, you can type R code just like you would in any R script, as explained in the previous section. However, in RMarkdown, you also have the option of running an entire code chunk at once by hitting the green triangle at the top-right of a given code chunk.\n\n\n\nKnitting the document. Once you have added a code chunk and/or some text, you are ready to compile or “Knit” the document. This is what generates the .html document.\n\nTo do so, click on the Knit button toward the top of the top-left window of Rstudio. After a few moments, this should open up a preview window displaying the compiled html file.\nIt will also save an actual .html file in your working directory (the same location on your computer where you have saved the .Rmd file)\nTry to locate this compiled .html file on your computer and open it. For most computers, .html files will open in your default web browser, such as Google Chrome or Safari.\nThis step is a common place where errors are detected and generated. Sometimes the compiling process fails due to errors in the R code in your code chunks or an error in the Markdown syntax. If your document fails to knit, the next step is to try to troubleshoot the error messages the compiling process generates. The best way to reduce and more easily detect errors is to “knit as you go.” Try to knit your document after each chunk of code you create."
  },
  {
    "objectID": "index.html#optional-setup-short-assignment",
    "href": "index.html#optional-setup-short-assignment",
    "title": "Experiments 2024",
    "section": "Optional Setup Short Assignment",
    "text": "Optional Setup Short Assignment\nBelow is an exercise that will demonstrate you are able to use R as a calculator, create R scripts, and create and compile R Markdown files.\nYou will submit three documents on Canvas:\n\nAn R script (.R) file with your code. Follow the best practices by titling your script and using # comments to explain your steps. This code should be clean. I should be able to run your code to verify that the code produces the answers you write down.\nAn .Rmd document and a compiled RMarkdown .html document that you get after “knitting” the .Rmd file. This should also have a title including your name and use text or # comments to explain your steps.\n\nThis video provides a brief overview of opening an R script and R Markdown file in RStudio. The notes above provide additional details.\n\nOptional Assignment Exercises\n\nCreate a .R script saved as “LastnameSetup1.R” (use your last name). Within this file, make sure to title it and provide your name.\n\nSet your working directory, and include the file pathway (within setwd()) at the top of your .R script\nDo the calculation 8 + 4 - 5 in your R script. Store it as an object with an informative name. Report the answer as a comment # below the code.\nDo the calculation 6 x 3 in your R script. Store it as an object with an informative name. Report the answer as a comment # below the code.\nAdd these two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer as a # below the code.\n\nIn this problem, we will just re-format what we did in the first problem in an R Markdown format. Create a .Rmd R Markdown file saved as “LastnameSetup1.Rmd.” Within this file, make sure to title it and provide your name.\n\nCreate a Markdown heading # Problem 2.1. Underneath this, create an R code chunk in which you do the calculation 8 + 4 - 5. Store it as an object with an informative name. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.2. Underneath this, create an R code chunk in which you do the calculation 6 x 3 in your R script. Store it as an object with an informative name. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.3. Underneath this, create an R code chunk in which you add the previous two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer in plain language below the code chunk.\nCreate a Markdown heading # Problem 2.4. Write down how you will complete your R assignments this semester. For example, if you have a personal laptop with R and RStudio on it, you will simply write “I will use my personal laptop.” If you don’t have a personal computer or laptop, please indicate where on campus or off-campus you will have regular access to a computer with R/RStudio to do your work. It is essential that you have regular access to a computer so that you will not fall behind in this course.\n\nCreate a compiled .html file by “knitting” the .Rmd file into a .html document. Save the file as “LastnameSetup1.html.”\n\nAll done! Submit the three documents on Canvas."
  },
  {
    "objectID": "01-Experimentation.html#what-are-experiments",
    "href": "01-Experimentation.html#what-are-experiments",
    "title": "1  Why Experiment?",
    "section": "1.1 What are experiments?",
    "text": "1.1 What are experiments?\nOur first discussion will be focused on elaborating on what we see as the goals of social science and how experiments fit into these goals.\nWe draw on the following readings\n\nGerber, A. and D.P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W.W. Norton. Chapter 1.\nAngrist, Joshua D. and Jorn-Steffen Pischke. Mostly Harmless Econometrics. Part One: Preliminaries: “Questions about Questions.” Available online here\nKinder, Donald R. and Thomas R. Palfrey. 1993. “On Behalf of an Experimental Political Science.” In Experimental Foundations of Political Science.\n\nSkim one of the following for a more skeptical take on experiments/randomized controlled trials\n\nTeele, Dawn. Field Experiments and Their Critics : Essays on the Uses and Abuses of Experimentation in the Social Sciences. New Haven: Yale University Press. Chapter 9 by Ian Shapiro or Chapter 2 by Susan Stokes\nStevenson, Megan. 2023. CAUSE, EFFECT, AND THE STRUCTURE OF THE SOCIAL WORLD. BU Law Review https://www.bu.edu/bulawreview/files/2023/12/STEVENSON.pdf\n\nWe will sketch out the answers to these questions as a group. Along the way, we will try to build a research design for a research question we come up with as a class.\n\nWhat are the goals of social science? What are examples of research questions that can be addressed with each goal?\n\nYour ideas …\n\nWhat makes an experiment an experiment? What are the goals of experimentation?\n\nYour ideas …\n\nWhat are some advantages of experimentation over other methods in political science?\n\nYour ideas …\n\nWhat are examples of different types of experiments?\n\nYour ideas …\n\nWhat are limitations of experiments? Can we experiment on everything?\n\nYour ideas …"
  },
  {
    "objectID": "01-Experimentation.html#getting-comfortable-with-r",
    "href": "01-Experimentation.html#getting-comfortable-with-r",
    "title": "1  Why Experiment?",
    "section": "1.2 Getting Comfortable with R",
    "text": "1.2 Getting Comfortable with R\nWe will use data from the article below, also provided as additional practice in section 2 of the course notes:\nThal, A. (2020). The desire for social status and economic conservatism among affluent Americans. American Political Science Review, 114(2), 426-442.\nThis study is an experiment where affluent Americans are randomly assigned to encounter Facebook posts in which others broadcast their economic success. These posts are designed in a way that encourages affluent respondents to view economic success as a means of achieving social status. The experiment includes a sample of 2010 affluent Americans– people who report household incomes in the top 10 percent of the U.S. income distribution.\n\nCausal Question: Does desire for social status influence economic views of affluent Americans?\nRandomization: Randomly assign respondents to view different fictional Facebook posts designed to signal different motivations\nOutcome: An economic conservatism index based on respondents’ support for decreasing “taxes on households making $150,000 or more a year,” support for decreasing the “taxes on money people make from selling investments, also referred to as capital gains,” and support for decreasing “government regulation of business and industry.”\nComparison: Average economic views between experimental conditions that vary in the type of social cues given.\n\n \n\n1.2.1 Dataframes in R\nKosuke Imai’s QSS Chapter 1.3.5 pgs. 20-25 discusses different ways to load data based on the file type.\n\nCommon file types include .csv, .RData, .dta (a Stata format), .sav (and SPSS format)\nYou want to match the function with the file type.\n\nFor .RData files, we can just use the load command. That function works the following way:\n\nload(\"status.RData\")\n\nAfter running the above code, an object will show up in your R environment.\n\nhead(status)\n\n        condition male   econcon\n2        Concrete    1 0.7500000\n3     Self-Esteem    1 1.0000000\n4         Placebo    1 0.6666667\n5     Self-Esteem    0 0.2500000\n6     Self-Esteem    0 1.0000000\n7 Social Approval    0 0.8333333\n\n\nWe also have a status.dta version of the file. To load this dataset, we could use the read.dta function which uses the library(foreign) package, a package uses for working with data types that are foreign to R.\n\nWhen working with a function from outside of “base R”– one that is located in a package, you always must open the package first using library() before using the function.\n\n\nlibrary(foreign)\nstatusdta &lt;- read.dta(\"status.dta\")\n\nIn addition to these dataset-specific functions, some people like to use the package rio which has a generic function import which can be used to load many file types.\n\nIf you do not have a package installed, the first time you use the package, you must first install it using install.packages(). By adding dependencies = T to this function, R will also automatically install any other packages that this package relies on to use.\n\n\ninstall.packages(\"rio\", dependencies=T)\n\n\nlibrary(rio)\nstatusrio &lt;- import(\"status.dta\")\n\nWe also have a status.csv file type. We can use read.csv() to load this file.\n\nstatuscsv &lt;- read.csv(\"status.csv\")\n\nAs an alternative, some people prefer to use the read_csv function that comes from the tidyverse package readr.\n\nIf you get an error saying you haven’t installed tidyverse, follow what we did above in installing the rio package, but this time, for tidyverse.\n\n\nlibrary(tidyverse)\nstatuscsv2 &lt;- read_csv(\"status.csv\")\n\nYOUR TURN: Not that we could also use import() to load the .csv version of the file. You can try that now:\n\n## Use import to load the csv file\n## Note: remember to use the appropriate library command\n\nEach of these processes will load an object in your R environment.\n\nNote that there may be minor differences in how the data load depending on the function used. For example, some may include an extra indexing variable with a unique number per row.\nIn addition, there may be differences in the class() of how a variable loads.\n\n\nLet’s explore the data.\n\nWe can view the data in a separate window using the View() command\n\n\nView(status)\n\n\n\nNote that the top of each column is a header with a variable name.\nThes variable names “belong” to the dataframe object status\n\nThat is, status is a dataframe, which means it has rows and columns.\n\nIn our case, every row represents a different survey respondent.\nThe corresponding values in each column represent the values a given respondent takes on a different variable in the dataset. For example, the respondent in the 8th row was in the Self-Esteem condition, took the value 1 on male, and the value .4166 on econcon.\n\n\nclass(status)\n\n[1] \"data.frame\"\n\n\nWe have three primary columns or “variables” or “vectors” in our data.\n\ncondition: Placebo, Concrete, Self-Esteem, Social Approval, Conspicuous Consumption\nmale: 1= male; 0= otherwise\neconcon: Economic views. Numeric variable from 0 to 1, with higher values reflecting more conservative views\n\nTo access a column name that exists within a dataframe (i.e., the column condition exists within the dataframe status), we generally use the syntax of the dataframename$columnname. The dataframe name is on the left of the dollar sign, and the column name is on the right.\nTo access the values of the condition column, we type:\n\nstatus$condition\n\nYOUR TURN: access the values in the econcon column.\n\n## Access the values in the econcon column\n\n\n\n1.2.2 Computing summary statistics of variables\nThis syntax (dataframename$columnname) applies when we want to compute summary statistics of specific columns within data frames.\nR has a number of functions (See QSS chapter 1.3.4) that can be used to summarize columns aka vectors.\n\nExamples: mean(), median(), range(), sd(), length(), table()\n\nTo apply these to a column within our dataframe, we similarly have to follow the syntax dataframename$columnname inside our function. For example, to find the range of the econcon variable, we write:\n\nrange(status$econcon)\n\n[1] 0 1\n\n\nYOUR TURN: Find the mean of this column.\n\n## Find the mean of the econcon column\n\nIn the real world, often our data include missing values, which R represents as an NA. When this happens, we add an argument to these common functions, na.rm=T which tells R to “remove” / ignore the NA values when computing the mean, range, standard deviation, etc. Not all functions allow this argument, so if you receive an error when trying to use it, it could be that the argument doesn’t work for that particular function\n\nrange(status$econcon, na.rm=T)\n\n[1] 0 1\n\n\nNot all functions allow this argument, so if you receive an error when trying to use it, it could be that the argument doesn’t work for that particular function. For example, the code below will generate an error because table() doesn’t have this argument.\n\ntable(status$econcon, na.rm=T)\n\nA common tool we may use to summarize variables is also the table() command, which will tell you how many observations (survey respondents) take on a particular value of a variable. Example:\n\ntable(status$condition)\n\n\n                Placebo                Concrete Conspicuous Consumption \n                    394                     391                     392 \n            Self-Esteem         Social Approval \n                    390                     375 \n\n\nWe see, for example, that 394 respondents were in the Placebo experimental condition.\nYOUR TURN: Use table on the male column and indicate how many males and females we have in the data.\n\n## Use the table command to indicate how many male and female respondents\n\n\n\n1.2.3 Relational operators\nIn experiments, we often don’t want to know these summary statistics for all respondents. Instead, often we want to know the summary statistics separately for those that belong to different subgroups of the sample.\nWe can use relational operators to help us isolate particular subgroups of data when conducting our analysis. We have several relational operators in R that evaluate logical statements:\n\n==, &lt;, &gt;, &lt;=, &gt;=, !=\nWe have a statement and R evaluates it as TRUE or FALSE\nNote that relational operators use a double == to evaluate logical equivalency. This is different from the single = that is sometimes used elsewhere in R, such as in arguments within functions (.eg., na.rm = T)\n\n\n## for each observation, does the value of condition equal \"Self-Esteem\"?\nstatus$condition == \"Self-Esteem\"\n\nFor some values, it returns TRUE because a respondent was in that condition. For others, it returns FALSE because a respondent was in a different condition.\n\nNote that R is very sensitive, including case sensitive. You want to make sure you enter the values (e.g., “Self-Esteem”) EXACTLY as they appear in the dataframe. Extra spaces, typos, wrong capitalization will all give you the wrong answer.\nNote that we use quotations around “Self-Esteem” because it is text. If instead we had a logical statement involving a numeric value, we would not need quotes.\n\n\nstatus$male == 1\n\nYOUR TURN: Use a logical statement to evaluate whether a given respondent takes the value “Placebo” as the condition variable.\n\n## Does condition equal Placebo?\n\nBy putting this logical statement within [ ], we are asking R to take the mean() of the variable staus$econ for the subset of observations for which a logical statement is TRUE.\n\nLet’s take the overall mean of the econcon variable\nThis represents the average economic conservatism for all respondents\n\n\nmean(status$econcon, na.rm=T)\n\n[1] 0.6633625\n\n\n\nLet’s take the mean of the econcon variable for those in the “Social Approval” condition (status$condition == \"Social Approval\")\nThis represents the average economic conservatism for respondents in the Social Approval condition\n\n\nmean(status$econcon[status$condition == \"Social Approval\"], na.rm=T)\n\n[1] 0.6904444\n\n\nYour TURN: Compare this to the mean of those in the Placebo condition.\n\n## Find the mean econcon for those in the Placebo condition\n\n\n1.2.3.1 Adding Boolean operators to relational statements\nInstead of a single relational statement, sometimes we may want to combine multiple relational operators into a single logical statement.\nFor example, we may want to find the average economic views for male respondents, only, in the social approval condition. We need to find those that are in the Social Approval condition and are male.\n\nIn R, we can use & and | to represent AND and OR statements\n\nFor example, this will evaluate the logical statement asking if a respondent is in the Social Approval condition AND is male.\n\nstatus$condition == \"Social Approval\" & status$male == 1\n\nFor example, this will evaluate the logical statement asking if a respondent is in the Social Approval condition OR the Placebo condition.\n\nstatus$condition == \"Social Approval\" | status$condition == \"Placebo\"\n\nJust like before, we can embed this entire statement into our [] to isolate these respondents when calculating descriptive statistics, such as average economic conservatism for these respondents.\n\nmean(status$econcon[status$condition == \"Social Approval\" \n                    & status$male == 1], na.rm=T)\n\n[1] 0.733631\n\n\nYOUR TURN: Find the average economic conservatism for respondents who are either in the Social Approval or Placebo conditions.\n\n### Mean econcon for respondents in Social Approval or Placebo condition\n\n\n\n1.2.3.2 Storing calculations as objects\nFor any of these calculations, you can store them as objects in your R environment by using the &lt;- assignment tool. You will always write the object name you desire to the left of this tool, and keep the calculations on the right.\n\nStoring these calculations can be useful because instead of needing to remember the raw number, you can just write the object name to retrieve the calculation.\n\nFor example we could save the mean economic views for respondents in the Social Approval condition as an object meanSocApp\n\nYou can name objects pretty much anything. You just want them to be relatively short, informative, and try to avoid special characters or words that have some other meaning in R (Example: you wouldn’t want to name an object range because that is already a function name in R.)\n\n\nmeanSocApp &lt;- mean(status$econcon[status$condition == \"Social Approval\"],\n                   na.rm=T)\nmeanSocApp\n\n[1] 0.6904444\n\n\nOnce you create an object, it should also show up in your R environment.\n\nLet’s do the same for the Placebo and Concrete conditions.\n\nmeanPlacebo &lt;- mean(status$econcon[status$condition == \"Placebo\"],\n                   na.rm=T)\nmeanPlacebo\n\n[1] 0.6340948\n\n\n\nmeanConcrete &lt;- mean(status$econcon[status$condition == \"Concrete\"],\n                   na.rm=T)\nmeanConcrete\n\n[1] 0.6647485\n\n\n\n\n\n1.2.4 Subsetting Dataframes\nThus far, we have used relational operators and boolean statements to isolate values within particular columns of a dataframe. We might also just want to simply cut down our whole dataframe (e.g., status) and create a new dataframe that contains only those rows relevant to a particular group of respondents.\n\nIn subsetting an entire dataframe, we retain all of the columns in the dataframe. (I.e., we will still have the columns condition, male and econcon). However, we will have a smaller number of rows.\n\nFor example, perhaps the researcher was interested in how the experiment worked for only respondents who are male. If we know we are going to conduct all of our analyses just on male respondents, it could be efficient for us to create a new, smaller dataframe that only includes rows where a respondent is male.\nTo do this, we will use the subset R command. It has the syntax newdataframe &lt;- subset(existingdataframe, logicalstatement). For example, let’s create a new dataframe maleonly that contains the rows from the existing dataframe status where male == 1, reflecting that a respondent is male.\n\nNote: In this function, we deviate from our previous syntax of using dataframe$columnname. This is because in the first argument, we tell R in which dataframe our columns are located.\n\n\nmaleonly &lt;- subset(status, male ==1)\n\nNote that this creates a new dataframe object in our environment. It just has a smaller number of observations (rows), reflecting that not all of our sample was male.\n\nWe can view this new dataframe using the same View() command as before.\n\nView(maleonly)\n\n\nNote that it looks very similar to the original status dataframe, but now all of the values in the male column are 1.\nWe can treat this new dataframe the same way as status going forward, in that we can use the maleonly$columnname syntax to summarize columns within the maleonly dataframe. For example, maleonly$econcon would represent the values that male respondents take on economic conservatism.\nYour TURN: Using the new dataframe, find the average economic conservatism for male respondents.\n\n## Using maleonly dataframe, find mean economic conservatism\n\n\n## Note the value's equivalence to\nmean(status$econcon[status$male == 1], na.rm=T)\n\n[1] 0.69486\n\n\nSubsetting data can be an efficient way to write code to avoid the need to repeat relational operators within functions when computing summary statistics. For example, in the first problem set, you may subset your data to include only people that prefer to watch Entertainment, another subset for those who prefer to watch Fox, and so on.\n\n\n1.2.5 Working outside of dataframes\nWhile much of our work in analyzing social science studies will exist within our dataframe objects, there are times where we may construct our own sets of objects that exist outside of dataframes.\nFor example, we created meanConcrete, meanPlacebo, and meanSocApp objects.\n\nThese represent the average economic conservatism for respondents in the Concrete, Placebo, and Social Approval conditions.\n\nTo retrieve each of these values, we could type them separately:\n\nmeanConcrete\n\n[1] 0.6647485\n\nmeanPlacebo\n\n[1] 0.6340948\n\nmeanSocApp\n\n[1] 0.6904444\n\n\nHowever, to be more efficient, we could also bind them together in a single object using the c() function. This function creates a vector.\n\nconditionmeans &lt;- c(meanConcrete, meanPlacebo, meanSocApp)\nconditionmeans\n\n[1] 0.6647485 0.6340948 0.6904444\n\n\nNow, to retrieve the means for all of these conditions, we can simply type and run conditionmeans.\nNote: Because conditionmeans does not exist within a dataframe, we don’t need a $ to access it. This is in contrast to a vector like econcon which solely exists within the dataframes status or maleonly.\nBinding together values can come in handy when writing up reports of an analysis or even for visualization.\nWhile we won’t go into detail on plotting in this session, we can see an example of plotting these three condition means at points 1,2, and 3 on a simple point-based plot() in R.\n\nplot(x = c(1,2,3),\n     y = conditionmeans)\n\n\n\n\nWe can spice up the plot with some aesthetics to make it more readable:\n\nplot(x = c(1,2,3),\n     y = conditionmeans,\n     xlim = c(.5, 3.5),\n     ylim=c(.6, .75),\n     xlab= \"Experimental Condition\",\n     ylab= \"Mean Economic Conservatism\",\n     main = \"Economic conservatism by condition\",\n     xaxt=\"n\")\naxis(1, c(1,2,3), c(\"Concrete\", \"Placebo\", \"Social\\n Approval\"),\n     tick=F)\n\n\n\n\nAnother approach would be to put the means into a dataframe. That type of format makes it easier to work with the ggplot interface, an alternative to plot\nThere are two ways to do this. First, we could manually create a dataframe using the conditionmeans we created above. In a second way, explored in the next section, we could use tidyverse functions to find the means.\n\nplotdf &lt;- data.frame(means = conditionmeans,\n                     groups = c(c(\"Concrete\", \"Placebo\", \"Social\\n Approval\")))\nplotdf\n\n      means            groups\n1 0.6647485          Concrete\n2 0.6340948           Placebo\n3 0.6904444 Social\\n Approval\n\n\n\nggplot(plotdf, aes(x=groups, y=means))+\n  geom_point()+\n  labs(y= \"Mean Economic Conservatism\", title=\"Economic Conservatism by Condition\",\n       x=\"Experimental Condition\")+\n  theme_minimal()\n\n\n\n\nHow did seeing a message about social approval influence economic attitudes?"
  },
  {
    "objectID": "02-CausalEffects.html#potential-outcomes-framework",
    "href": "02-CausalEffects.html#potential-outcomes-framework",
    "title": "2  Causal Effects",
    "section": "2.1 Potential Outcomes Framework",
    "text": "2.1 Potential Outcomes Framework\nTo make causal claims, we compare two states of the world and their potential outcomes:\n\\(Y_i(d)\\)\n\nWhat is \\(Y_i(0)\\)?\nWhat is \\(Y_i(1)\\)?\n\n\\(i\\) refers to individual subjects from \\(i = 1\\) to N.\n\\(d\\) is the treatment indicator\n\\(d_i\\) refers to whether the subject is treated: \\(d_i = 1\\) or \\(d_i = 0\\)\n\\(D_i\\) refers to a hypothetical treatment allocation\n\n\nA causal “treatment effect” is then the difference in these potential outcomes:\n\n\\(\\tau_i\\) = \\(Y_i(1)\\) - \\(Y_i(0)\\)\n\n FEDAI Table 2.1\nThe treatment effect is the difference between two states of the world: one which a unit receives treatment, and another in which it does not.\n\n2.1.1 Average Treatment Effect\nThe average treatment effect then is the mean of these individual treatment effects:\n\nEstimand: On average, how much outcomes would change if all units go from untreated to treated.\n\n\\[\\begin{align*}\nATE &= \\frac{1}{N} \\sum_{i=1}^N \\tau_i \\\\\n&= \\mu_{Y(1)} -\\mu_{Y(0)} \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N Y_i (1) - \\frac{1}{N} \\sum_{i=1}^N Y_i (0) \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N (Y_i (1)-Y_i (0))\\\\\n&= E[Y_i(1) - Y_i(0)]\\\\\n\\end{align*}\\]\nATE \\(= \\frac{1}{N} \\sum_{i=1}^N \\tau_i\\) is what we want to describe a causal effect, but in real life, we have problems. What are they?\n\n\nTry on your own, then expand for the answer.\n\nWe only observe one potential outcome.\n\n\\(Y_i = d_iY_i(1) + (1-d_i)Y_i(0)\\)\n\n(Unless we are in Groundhog Day)"
  },
  {
    "objectID": "02-CausalEffects.html#fundamental-problem-of-causal-inference",
    "href": "02-CausalEffects.html#fundamental-problem-of-causal-inference",
    "title": "2  Causal Effects",
    "section": "2.2 Fundamental Problem of Causal Inference",
    "text": "2.2 Fundamental Problem of Causal Inference\nWe only observe one potential outcome: \\(Y_i\\).\n\n\\(Y_i = d_iY_i(1) + (1-d_i)Y_i(0)\\)\n\n(Unless we are in Groundhog Day)\nThe fundamental problem of causal inference is that we can only observe one potential outcome, the outcome in this case, under the state of the world \\(Y_i(0)\\) where the play unfolded as it did in the video.\n\nIt is impossible to observe the actual causal effect of any of the above: \\(Y_i(1) -Y_i(0)\\)"
  },
  {
    "objectID": "02-CausalEffects.html#identification-strategy",
    "href": "02-CausalEffects.html#identification-strategy",
    "title": "2  Causal Effects",
    "section": "2.3 Identification strategy",
    "text": "2.3 Identification strategy\nWe cannot observe the ideal actual causal effect. Instead, we will frame our exercise on the premise that we are randomly sampling our \\(i's\\) from a population. We then will create an identification strategy.\n\n“Ideas that enable researchers to use observable quantities (e.g., sample averages) to reveal parameters of interest (e.g., average treatment effects)” (Gerber and Green 2012, 34)\nInstead of observing the actual individual causal treatment effect and actual ATE, we develop an estimator for this quantity using the sample averages.\n\nA few definitions:\n\nThe sample average is a random variable, a quantity that varies from sample to sample.1\nExpected value is the average outcome of a random variable weighted by its probability of occurrence.\nGood news: Under random sampling, the expected value of a sample average is the population average.\nSimilarly, the expectation of a randomly selected observation from the population is the population mean.\nEven though we have a sample, under random sampling, our sample will be unbiased. On average, it’s true.\n\nWhen the expected value of a sample estimate is equal to the population parameter \\(E[\\hat{\\theta}] = \\theta\\), this means our estimator is “unbiased.”\n\n\nExpectation\n\\[\\begin{align*}\nE[X]=\\sum x Pr[X=x]\n\\end{align*}\\]\n\nwhere \\(Pr[X=x]\\) denotes the probability that \\(X\\) takes on the value \\(x\\), and where the summation is taken over all possible values of \\(x\\). Think of this like a weighted average.\n\nExample: \\(E[Y_i(1)]\\) is the expected value of the treated potential outcome of a subject who is randomly sampled.(It will equal the average value of all possible values.)\n\nWhat is the value of \\(E[Y_i(1)]\\) in this example?\n\n FEDAI Table 2.1"
  },
  {
    "objectID": "02-CausalEffects.html#difference-in-means-estimator",
    "href": "02-CausalEffects.html#difference-in-means-estimator",
    "title": "2  Causal Effects",
    "section": "2.4 Difference in Means Estimator",
    "text": "2.4 Difference in Means Estimator\nIn the real world, we follow this process for causal identification:\n\nOur motivation: Find quantities that represent the population parameters (\\(\\theta\\))\nOur problem: We often only get a sample of the population and can only observe one potential outcome for any unit in our sample\nGoal: Get unbiased estimators for the population\nDefinition of unbiasedness: \\(E[\\hat{\\theta}] = \\theta\\)\n\nSuppose \\(D_i\\) were randomly assigned such that \\(m\\) subjects assigned to treatment and \\(N-m\\) subjects assigned to control.\n\\[\\begin{align*}\n\\widehat{ATE} &= \\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i \\\\\n\\end{align*}\\]\nIs the difference in means estimator an unbiased estimate for the ATE? How can we find out?\nWe take the expected value:\n\\[\\begin{align*}\nE[\\widehat{ATE}] &= E[\\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i ]\\\\\n&= \\frac{1}{m}\\sum_1^m E(Y_i) - \\frac{1}{N-m}\\sum_{m+1}^{N} E(Y_i ) \\\\\n&= \\frac{E(Y_1) + E(Y_2) +...+E(Y_m)}{m} -  \\frac{E(Y_{m+1}) + E(Y_{m+2}) +...+E(Y_N)}{N-m}\\\\\n&= \\frac{m * E[Y_i(1 | D_i = 1)]}{m} - \\frac{(N-m)* E[Y_i(0) | D_i = 0]}{N-m}\\\\\n&= E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] \\\\\n%&= E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]=ATE\n\\end{align*}\\]\nIs the final statement equivalent to the ATE?\n\nWe want our final statement to be \\(E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]\\)=ATE\nOur final statement is: \\(E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0]=E[\\widehat{ATE}]\\)\n\nUnder what conditions can we get those two statements to look the same?\n\n\nWell, let’s look into some rules of expectation.\n\n\\(E[Y|X] = E[Y]\\) if Y and X are independent.2\n\nOur final statement can be simplified when treatment assignment is independent of potential outcomes:\n\n\\(E[Y_i(1) |D_i = 1] = E[Y_i(1) |D_i = 0] = E[Y_i(1)]\\)\n\\(E[Y_i(0) |D_i = 0] = E[Y_i(0) |D_i = 1] = E[Y_i(0)]\\)\n\nWhen does this occur? Random assignment of treatment!!\nPutting this together, under random assignment:\n\\[\\begin{align*}\nE[\\widehat{ATE}] &= E[\\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i ]\\\\\n&= \\frac{1}{m}\\sum_1^m E(Y_i) - \\frac{1}{N-m}\\sum_{m+1}^{N} E(Y_i ) \\\\\n&= E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] \\\\\n&= E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]\\\\\nE[\\widehat{ATE}] &= ATE\n\\end{align*}\\]\nWhy Experiments\nOne approach for addressing the fundamental problem of causal inference is to simulate two potential states of the world through random assignment: Randomized Controlled Trials / Experiments\nExperiments approximate factual vs. counterfactual comparison\n\nWe randomly assign one group to receive a “treatment” and another not to receive a treatment (the control)\nUsing what we learned above, when treatment assignment is randomized, the only thing that distinguishes the treatment group from the control group in expectation, besides the treatment itself, is chance.\nThis allows us to use a simple differences in means estimator in experiments to estimate our average treatment effects."
  },
  {
    "objectID": "02-CausalEffects.html#overview-of-identification-assumptions",
    "href": "02-CausalEffects.html#overview-of-identification-assumptions",
    "title": "2  Causal Effects",
    "section": "2.5 Overview of identification assumptions",
    "text": "2.5 Overview of identification assumptions\nWhat if we can’t guarantee random assignment?\nExample: Selection into treatment\nWhat if we didn’t have the independence? Subtract and add \\(E[Y_i (0) | D_i=1]\\) to help us illustrate a type of bias that may occur.\n\\(E[Y_i (1) | D_i=1]-E[Y_i (0) | D_i=0] =\\)\n\\(\\underbrace{E[Y_i (1) | D_i = 1] - E[Y_i (0) | D_i=1]}_{\\text{Average treatment effect for the treated}} + \\underbrace{E[Y_i (0)|D_i=1]-E[Y_i (0)| D_i=0] }_{\\text{Selection bias}}\\)\nIn observational studies, where assignment into treatment is not random, the second term “Selection bias” may not be zero.\n\nE.g., suppose we want to know the effect of minimum wage laws on unemployment.\nLaws aren’t randomly assigned\nPossible that states where unemployment (outcome) is lower are less likely to see minimum wage laws passed relative to states where unemployment is higher. If so, the potential outcomes \\(Y_i(0)\\) of states that would hypothetically be treated or untreated would not be the same.\n\nAssumptions\nTo “identify” the average treatment effect, we need\n\nProbability of treatment of all units is between 0 and 1\nIgnorability: \\(Y_i(1), Y_i(0) \\perp D_i\\) (random assignment)\nNon-interference: \\(Y_i(d_1, d_2, ..., d_n) = Y_i(d)\\), \\(d_i = d\\)\nExcludability: if \\(Y_i(z, d)\\) where z \\(\\in [0, 1]\\) and \\(d \\in [0, 1]\\), \\(Y_i(1, d) = Y_i(0, d)\\)\n\nLet’s put these into plain words."
  },
  {
    "objectID": "02-CausalEffects.html#application-in-r",
    "href": "02-CausalEffects.html#application-in-r",
    "title": "2  Causal Effects",
    "section": "2.6 Application in R",
    "text": "2.6 Application in R\nArticle: “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” by Bertrand and Mullainathan (2004)\n\nResearch Question: Does race influence hiring decisions?\nWhat are the potential outcomes?\nWhat is the approach? Audit study: “send fictitious resumes to help-wanted ads in Boston and Chicago newspapers.\n\nTreatment: Manipulate perceived race: resumes randomly assigned African-American- or White-sounding names.\nOutcomes: Does the resume get a callback?\n\n\nHow should we estimate the average treatment effect?\n\n2.6.1 Loading the data\nWe will use data from Imai (2017) Chapter 2.\nLet’s load the data. Note: When we have variables that are text-based categories, we may want to tell R to treat these “strings” of text information as factor variables, a particular type of variable that represents data as a set of nominal (unordered) or ordinal (ordered) categories. We do this with the stringsAsFactors argument.\n\nresume &lt;- read.csv(\"resume.csv\", stringsAsFactors = T)\n\n\nresume &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\",\n                   stringsAsFactors = T)\n\nVariables and Description\n\nfirstname: first name of the fictitious job applicant\nsex: sex of applicant (female or male)\nrace: race of applicant (black or white)\ncall: whether a callback was made (1 = yes, 0 = no)\n\nThe data contain 4870 resumes and 4 variables.\n\nnrow(resume) # number of rows\n\n[1] 4870\n\nncol(resume) # number of columns\n\n[1] 4\n\ndim(resume) # number of rows and columns\n\n[1] 4870    4\n\n\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0\n\n\n\n\n2.6.2 Variable classes\nWe can check the class of each variable: Look, we have a new type, a “factor” variable.\n\nclass(resume$firstname)\n\n[1] \"factor\"\n\nclass(resume$sex)\n\n[1] \"factor\"\n\nclass(resume$race)\n\n[1] \"factor\"\n\nclass(resume$call)\n\n[1] \"integer\"\n\n\nRules of Thumb\n\nUsually, we want character variables to store text (e.g., open-ended survey responses)\nWe want numeric variables to store numbers.\nUsually, we want factor variables to store categories.\n\nWithin R, factor variables assign a number to each category, which is given a label or level in the form of text.\nCategories might be ordinal or “ordered” (e.g., Very likely, Somewhat likely, Not likely) or\nUnordered (e.g., “male”, “female”)\nR won’t know if a factor variable is ordered or unordered. Alas, we have to be smarter than R.\nR might think you have a character variable when you want it to be a factor or the reverse.\n\nThat’s when as.factor() and as.character() are useful.\n\n\nAlways check class() to find out the variable type\n\n\n\n2.6.3 Exploring Treatment and Control Groups\nWe are going to use several different approaches to calculate our difference in means between treatment and control to help us explore R’s capabilities and common computational approaches.\nWe can use the table command to see how many observations in our data fall into each category or numerical value.\n\n## Example: how many black vs. white sounding resumes\ntable(resume$race)\n\n\nblack white \n 2435  2435 \n\n\nAs mentioned, factor variables have levels:\n\nlevels(resume$race)\n\n[1] \"black\" \"white\"\n\n\nWe can also use the table command to show a crosstabulation: a table that displays the frequency of observations across two variables. Because our outcome variable call is dichotomous and we are interested in the rates of callbacks, we might use a table to display this information. (For outcomes that are continuous, the table approach is less useful.)\n\n## Example: how many black vs. white sounding resumes by call backs\n## We can label the two dimensions of the table with the =\ntable(calledback = resume$call, race = resume$race)\n\n          race\ncalledback black white\n         0  2278  2200\n         1   157   235\n\n\nSometimes we will want to show the proportion instead of the frequency using prop.table\n\n## Example: proportion black vs. white sounding resumes by call backs\n## Convert to proportion\nprop.table(table(calledback = resume$call, race = resume$race), margin = 2) # 1 for row sum, 2 for col\n\n          race\ncalledback      black      white\n         0 0.93552361 0.90349076\n         1 0.06447639 0.09650924\n\n\nHow can we interpret this crosstabulation?\n\n\n2.6.4 Means with Relational Operators\nGoal: Compare callback rates for white sounding names to black sounding names, so we need to be able to filter by race.\nGood news: We have several relational operators in R that evaluate logical statements:\n\n==, &lt;, &gt;, &lt;=, &gt;=, !=\nWe have a statement and R evaluates it as TRUE or FALSE\n\n\n## for each observation, does the value of race equal \"black\"?\nresume$race == \"black\"\n\nBy putting this logical statement within [ ], we are asking R to take the mean() of the variable resume$call for the subset of observations for which this logical statement is TRUE.\n\nmean(resume$call[resume$race == \"black\"])\n\n[1] 0.06447639\n\n\nUltimately, each of these paths has led us to a place where we can estimate the average treatment effect by calculation the difference in means: the difference in callback rates for black and white applicants.\nWe said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\)\n\nate &lt;- mean(resume$call[resume$race == \"black\"]) - \n  mean(resume$call[resume$race == \"white\"])\nate\n\n[1] -0.03203285\n\n\nHow can we interpret this? Do white applicants have an advantage?\n\n\n2.6.5 Means with tidyverse\nThe tidyverse offers a suite of R functions and a different grammar or syntax of coding. Some people prefer this to the “base R” codes we did above. To use this suite, first install the tidyverse package:\nWhen you install a package, this is like downloading an app to your phone. You only have to do it one time.\n\ninstall.packages(\"tidyverse\")\n\nAfter you have a package installed, much like an app on your phone, you then need to open it before using it in R. To do so, use the library() command.\n\nlibrary(tidyverse)\n\nThe tidyverse works through these piping %&gt;% operators. We can read it from left to right. Take our dataset resume, group the data by race, and within each racial group, summarize the data by taking the mean call back rate.\n\nresume %&gt;%\n  group_by(race) %&gt;%\n  summarise(means = mean(call))\n\n# A tibble: 2 × 2\n  race   means\n  &lt;fct&gt;  &lt;dbl&gt;\n1 black 0.0645\n2 white 0.0965\n\n\nWe could go a step further to calculate the ATE.\n\nate &lt;- resume %&gt;%\n  group_by(race) %&gt;%\n  summarise(means = mean(call)) %&gt;%\n  ungroup() %&gt;%\n  spread(race, means)%&gt;%\n  mutate(diff = black - white)\n\nate\n\n# A tibble: 1 × 3\n   black  white    diff\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0645 0.0965 -0.0320\n\n\n\n\n2.6.6 ATE with linear regression\nLinear regression also offers a way to calculate the conditional means and difference in means between two groups. In R, we use lm() for this. The syntax is lm(y ~ x, data = mydataframe).\n\nfit &lt;- lm(call ~ race, data =resume)\n\nWe can look at the coefficient results only.\n\nfit$coefficients\n\n(Intercept)   racewhite \n 0.06447639  0.03203285 \n\n\nIn a regression of this form, the intercept represents the mean of the reference category, in this case, the callback rate for Black applicants. The coefficient on racewhite represents the difference in means between the reference category and this group. I.e., going from a Black applicant (the reference category) to a white applicant, on average, increases call backs by 3.2 percentage points.\n\n\n2.6.7 Subsetting data in R\nMaybe we are interested in differences in callbacks for females. One approach for looking at the treatment effect for female applicants, only, is to subset our data to include only female names.\n\nTo do this, we will assign a new data.frame object that keeps only those rows where sex == \"female\" and retains all columns\nBelow are two approaches for this subsetting, one that uses brackets and one that uses the subset function\n\n\n## option one\nfemales &lt;- resume[resume$sex == \"female\", ]\n## option two using subset()- preferred\nfemales &lt;- subset(resume, sex == \"female\")\n\nNow that we have subset the data, this simplifies estimating the ATE for female applicants only.\nWe said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\)\n\nate.females &lt;- mean(females$call[females$race == \"black\"]) -\n  mean(females$call[females$race == \"white\"])\nate.females\n\n[1] -0.03264689\n\n\nQuestion: Is this an unbiased estimate of the average treatment effect?\n\n\nTry on your own, then expand for the answer.\n\nThis is an example of a “Conditional Average Treatment Effect.” Generally, because gender is a pre-treatment factor, we can condition on it and get unbiased estimates for the average treatment effect within a particular gender group.\n\nRandom assignment of treatment means that in expectation, we should have about equal proportions of female applicants in each treatment group, ruling out the potential for selection bias.\n\n\n\n\n2.6.8 Additional Practice\nWe will use data from the article below:\nThal, A. (2020). The desire for social status and economic conservatism among affluent Americans. American Political Science Review, 114(2), 426-442.\nIn the experiment, affluent Americans are randomly assigned to encounter Facebook posts in which others broadcast their economic success. These posts are designed in a way that encourages affluent respondents to view economic success as a means of achieving social status. The experiment includes a sample of 2010 affluent Americans– people who report household incomes in the top 10 percent of the U.S. income distribution.\n\nCausal Question: Does desire for social status influence economic views of affluent Americans?\nRandomization: Randomly assign respondents to view different fictional Facebook posts designed to signal different motivations\nOutcome: An economic conservatism index based on respondents’ support for decreasing “taxes on households making $150,000 or more a year,” support for decreasing the “taxes on money people make from selling investments, also referred to as capital gains,” and support for decreasing “government regulation of business and industry.”\nComparison: Average economic views between experimental conditions that vary in the type of social cues given.\n\n\n\nLet’s load the data! Here, note that the data file is in a .RData format instead of .csv. This means that instead of using read.csv, we should use a function to load the data that is suitable for the .RData format. This will be load. That function works the following way:\n\nload(\"status.RData\")\n\nAfter running the above code, an object will show up in your R environment.\n\nhead(status)\n\n        condition male   econcon\n2        Concrete    1 0.7500000\n3     Self-Esteem    1 1.0000000\n4         Placebo    1 0.6666667\n5     Self-Esteem    0 0.2500000\n6     Self-Esteem    0 1.0000000\n7 Social Approval    0 0.8333333\n\n\nThe data include the following variables\n\ncondition: Placebo, Concrete, Self-Esteem, Social Approval, Conspicuous Consumption\nmale: 1= male; 0= otherwise\neconcon: Economic views. Numeric variable from 0 to 1, with higher values reflecting more conservative views\n\nPractice:\n\nHow many people are in each condition?\nWhat is the average treatment effect between the Placebo and Social Approval conditions?\n\n\n\nTry on your own, then expand for the answer.\n\n\n## Number of observations\ntable(status$condition)\n\n\n                Placebo                Concrete Conspicuous Consumption \n                    394                     391                     392 \n            Self-Esteem         Social Approval \n                    390                     375 \n\n## tidy\ngroupmeans &lt;- status %&gt;%\n  group_by(condition) %&gt;%\n  summarise(means = mean(econcon)) %&gt;%\n  ungroup %&gt;%\n  spread(condition, means) \ngroupmeans$`Social Approval` - groupmeans$Placebo\n\n[1] 0.05634969\n\n## relational operators\nate &lt;- mean(status$econcon[status$condition == \"Social Approval\"]) - mean(status$econcon[status$condition == \"Placebo\"]) \nate\n\n[1] 0.05634969\n\n## regression\nfit &lt;- lm(econcon ~ condition, data = status)\nfit$coefficients[\"conditionSocial Approval\"]\n\nconditionSocial Approval \n              0.05634969 \n\n\n\nAdditional Review Questions\n\nWhat is this quantity \\(E[Y_i (1) − Y_i (0)]\\) conceptually?\nWhat is the fundamental problem of causal inference?\nHow can we find out if our estimates are unbiased? (What process do we need to do?)\nWith randomization, why is \\(E[Y_i (1)] = E [Y_i (1)|D_i = 1]\\)?\nWhat other assumptions do we need to estimate the ATE in an unbiased way using differences in means?"
  },
  {
    "objectID": "02-CausalEffects.html#footnotes",
    "href": "02-CausalEffects.html#footnotes",
    "title": "2  Causal Effects",
    "section": "",
    "text": "Note: other books may approach this slightly differently by defining a Sample ATE, taking \\(D_i\\) (treatment status) to be the random variable, and \\(Y_i(1)\\) as fixed within a sample.↩︎\nSee video for help on law of iterated expectations↩︎"
  },
  {
    "objectID": "03-ExperimentalDesign.html#designing-an-experiment",
    "href": "03-ExperimentalDesign.html#designing-an-experiment",
    "title": "3  Experimental Design",
    "section": "3.1 Designing an Experiment",
    "text": "3.1 Designing an Experiment\nFour main ingredients in an experimental design\n\nRecruitment of participants\nRandomization of treatment - means people in treatment and control groups will be similar\nDelivery of treatment (intervention)\nMeasurement of outcomes\n\nWhat does an experimental design test?\nBroadly “causal effects”: More specifically:\n\nFrom Mutz: Experiments are designed to answer the question, “If x changes, how should y be expected to change?”\n\nGoal of experimental treatment is to create variation in the independent variable in the direction (or directions) intended by the researcher.\n\nFrom Salganik: “Narrowly focused experiments answer a much more specific question: What is the average effect of this specific treatment with this specific implementation for this population of participants at this time?”\n\nHow can we evaluate experiments?\n\n3.1.1 Validity\n“Validity refers to the extent to which the results of a particular experiment support some more general conclusion.”\n\nStatistical conclusion validity- correctness of statistical analysis\nInternal validity- correctness of procedures\nConstruct validity- match between data and theoretical constructs\nExternal validity- how can results generalize to other situations\n\n\nWhat makes a good treatment? Does it require realism?\n\nYour ideas …\n\nHow does our excludability assumption factor into this?\n\nYour ideas …\n\nWhat does it mean to say a treatment is generalizable?\n\nYour ideas …\n\nHow can we increase engagement with our experiments?\n\nYour ideas …\n\nWhat is a manipulation check, and what role does it serve?\n\nYour ideas …\n\n\n\n3.1.2 Design Space for Experiments\n Figure 4.1\n\nWhat are the tradeoffs between digital vs. analog experiments?\n\nYour ideas …\n\nWhat are the tradeoffs between lab vs. field experiments?\n\nYour ideas …\n\n\n\n3.1.3 Types of Designs\n\nWhat are the strengths and weaknesses of different types of designs?\n\nYour ideas …"
  },
  {
    "objectID": "03-ExperimentalDesign.html#using-qualtrics",
    "href": "03-ExperimentalDesign.html#using-qualtrics",
    "title": "3  Experimental Design",
    "section": "3.2 Using Qualtrics",
    "text": "3.2 Using Qualtrics\nEveryone at Rutgers gets a free Qualtrics account. Qualtrics provides a user-friendly interface for designing online surveys and survey experiments.\nWe will walk through how to design a simple survey experiment on the platform.\n\nGo to the Rutgers Qualtrics site. The first time you use this you might have to initialize your account.\nClick on “Create a new project” and select “Survey” from scratch and get started.\nGive the project an informative name like “Experimental methods demo.” We will start with a blank survey project.\n\nThis should take you to a landing page that looks something like this:\n\nWhen running an academic survey, generally, the first survey question should be a consent form.\n\nRutgers has template consent forms here. For a survey, you may want the Online survey questionnaire consent form. The text of the consent form has to be approved by the IRB.\n\nAfter the consent form, you might ask respondents a set of “pre-treatment” questions, such as demographics, attention checks, etc. These are things you want to know about respondents prior to when they enter your experiment.\n\nYou can organize these questions into different “blocks.” Blocks make it easier to move groups of questions up and down the survey, randomize the order of questions people see within a given block, or branch people to see only one of a set of blocks. We will get to this later.\n\nNow we are ready to program an experiment. There are many ways to do this, but we will choose a couple of common approaches.\n\nIn general, programming the experiment will involve 1) entering experimental treatments and questions into the survey interface we are currently working in; and 2) building a randomizer in the survey flow.\nTo prepare to program your experiment, you should have a sense of how many unique experimental conditions you have.\n\nIf you have a relatively small number of experimental conditions (e.g., 2-4), an easy way to program the experiment is to manually create a unique block for each condition.\nIf you have a larger number of experimental conditions (e.g., if you are manipulating several things at once across conditions), you might consider integrating piped text or another approach to avoid the need to manually create all of your experimental conditions.\nIf you have a very large number of conditions or need to adjust the randomization in a more complex way (e.g., control the specific probabilities that certain conditions appear), you may need to integrate JavaScript code to help with randomization.\n\n\n\n3.2.1 Experimental Design with Vignette Experiment\nFor this example, we will use the experimental design from “Public Opinion and Foreign Electoral Intervention” by Michael Tomz and Jessica Weeks, published in the American Political Science Review in 2020. The article is here.\nThey “hypothesize that American tolerance of foreign intervention should depend on the type of intervention and the intended beneficiary. We distinguish three modes of interference: verbal endorsements, threats, and operations.”\n\nEndorsements occur when foreign countries express their opinions about candidates.\nThreats combine an endorsement with a promise of future reward or threat of future punishment, such as threatening to downgrade future relations if the preferred candidate loses.\nOperations [are] when foreign powers undertake efforts such as spreading embarrassing information about a candidate, hacking into voting systems, or donating money to an election campaign\n\nThese are contrasted with a comparison of staying out of the election.\nHypotheses\n\n“We predict that citizens will be most concerned about operations such as hacking into voting systems or donating money, as these directly advantage the favored candidate and involve behavior the U.N. has classified as impermissible interference in the internal affairs of another nation. Americans should be more tolerant of threats and most tolerant of endorsements, which could be seen as legitimate and harmless expressions of opinion that do not intrude on American sovereignty.”\n“We also hypothesize that revelations of foreign intervention will generate polarized partisan responses. . . we anticipate that American voters will disapprove more strongly of foreign meddling on behalf of political opponents, than of foreign meddling to assist their own party.”\n\nTable 1 in the paper displays the experimental design used to test the hypotheses. We will program the primary manipulation, which varies the endorsement, threat, operation, or stay out conditions. For now, we will fix the country to be China, the candidate to be the Democratic candidate, and the operation to be donating money and 100% certainty it was China.\n\n\n3.2.2 Unique blocks per experimental condition\nIn this approach, we will create a separate block for each unique experimental vignette. In our case, we will create a block with a Text/Graphic question type. We paste in our experimental text and give the block and question informative labels. The question name will be the name of the variable for the question when you eventually load the data.\n\nFor each condition, create a new block.\nSee the four blocks below, one for each condition\n\n   \nAfter creating each experimental block, we can then add a new block with our outcome questions. Go ahead and add 1-2 outcome questions so that you have an example.If your outcome condition text is specific to each treatment condition, you can create outcome questions within the experimental blocks.\nFinally, after that, you may have some last demographic questions. You can put those in yet another last block.\nOnce you have created all your blocks, you can now go to the survey flow. To do that, click on the icon in the left side of the survey landing page.\n\nYour survey flow should look something like this. We are now ready to add randomization so that each respondent only sees one of our experimental blocks, randomly assigned.\n\n\n\n3.2.3 Adding a randomizer in the survey flow\nWithin the survey flow, add a randomizer underneath the consent form.\n\nUnder the randomizer, add an embedded data field with an informative label for your treatment (e.g., “treat”)\n\nCreate a unique value for each of your treatment conditions\nMake sure the randomizer is set to show just one of these values.\n\n\n\nAs people go through the survey, under the hood of qualtrics, they will be assigned one of your experimental condition values. This embedded data field will show up as a variable in the data you download. However, we need one more step to make sure people only see the experimental block that corresponds to their embedded data field. This works through branching.\n\nAbove each experimental block, add a “Branch” object\nBranch people using embedded data. Set the condition to match each embedded data value, and then move the experimental blocks underneath the appropriate branch.\n\nYou can also duplicate the outcome block and put them underneath the corresponding treatment blocks.\n\nHit apply to make sure the survey flow saves.\n\n\nA last thing we often want to do is add a branch underneath the consent form to end the survey for anyone who does not consent to take the survey. The specific end-of-survey message you provide may be different depending on which company you use to recruit respondents. For now, we will use a default end-of-survey message.\n\nYou are now ready to “Preview” your survey!\n\n\n3.2.4 Using Piped Text in Randomization\nWe could complicate the randomization more so than we have done so far. For example, in Tomz and Weeks (2020), they do not fix the country to be China. Instead, they randomly vary this to be China, Pakistan, or Turkey for each respondent. We could build this added treatment arm into our design through “piped text.”\n\nGo back into your survey data flow. Create a second randomizer towards the top of the survey. Create a new embedded data field with an informative label, like country. Create unique fields for each of our country names. Hit apply.\n\n\n\nNow, in addition to being assigned a treatment condition, everyone is also independently randomly assigned a country. We now need to make sure the text they see reflects both their treatment condition and country.\n\nWe could create 4 X 3 experimental blocks to reflect these dual randomizations.\nInstead, we are going to integrate the second treatment into the four blocks we have already programmed– just to save us time.\n\nBack in the survey landing page, click on every single block of text or question that includes the word “China.” In place of “China” click on the “piped text” option. Set the piped text to be the “country” embedded data field. Here is an example. Note: you need to do this for every single mention of country.\n\n\nWe could complicate our design even further by adding additional piped text randomization to vary whether it is the Democratic vs. Republican candidate, the percent certainty about the country involved, and the type of operation. This would all involve adding additional randomizers and/or branching in the survey flow, along with piped text in the experimental blocks.\n\n\n3.2.5 Data and Analysis\nOnce you have a draft of your survey programmed, you will want to “preview” the survey from the perspective of a respondent by clicking “preview” in the survey landing page.\n\nRepeat this a few times to see if things seem to be working properly. After that, you can do a few more steps to test your survey.\n\nOption 1: Fake Data. In the survey landing, go to Tools -&gt; Generate test responses. This will automatically generate fake/simulated responses to your survey. This can allow you to check how the data will download, see if you can load it into your preferred statistical software and access the variables in the way you imagined, and check if the randomization appears to work properly.\nOption 2: Get a distribution link for your survey, and send the link to your friends and colleagues to help you test the survey from a respondent’s perspective.\n\nGo to Distributions -&gt; Get a single reusable link\nNote: once you click this, your survey is now published and “active.” To make any future changes to your survey, on the survey landing you will have to click “Publish.”\n\n\nWith both of these options, your survey will start to populate responses in the Data and Analysis section of Qualtrics. Click on this now that you have done one or both of these steps.\nThis will give you an overview of the responses and number of recorded responses.\nThis is also where we can download the data. Go to Export and Import - &gt; Export Data.\n\nDownload the data as a csv file if you plan to work in R.\nIf you click on “More options,” you can export randomized viewing order as well as other features you may want to toggle on or off.\n\nIf you open up your .csv file in a spreadsheet software like Excel, you will notice that the first row contains your question names as variables. The next two rows are more information about the questions, including the question wording. The actual responses start in row 3.\n\nIf you load the csv file into R as is, you would want to delete the first two rows from your dataframe (the first row will automatically be treated as a header in R). The R package qualtRics has a function that will do this for you.\nAlternatively, you can delete rows 2-3 from the spreadsheet software before loading it into R.\nSave the .csv file with an informative name in the working directory where you store files to work on in R.\n\nLoad the data into R.\n\n## my data are in a /data/ subfolder of my working directory\nexpdemo &lt;- read.csv(\"data/expdemo.csv\")\nexpdemo &lt;- expdemo[-c(1:2),] # remove first two rows\nnrow(expdemo)\n\n[1] 500\n\n\n\nlibrary(qualtRics)\n## my data are in a /data/ subfolder of my working directory\nexpdemo &lt;- read_survey(\"data/expdemo.csv\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  .default = col_character(),\n  IPAddress = col_logical(),\n  Progress = col_double(),\n  `Duration (in seconds)` = col_double(),\n  Finished = col_logical(),\n  RecipientLastName = col_logical(),\n  RecipientFirstName = col_logical(),\n  RecipientEmail = col_logical(),\n  ExternalReference = col_logical(),\n  LocationLatitude = col_double(),\n  LocationLongitude = col_double(),\n  UserLanguage = col_logical(),\n  manipcheck_year = col_double()\n)\nℹ Use `spec()` for the full column specifications.\n\nnrow(expdemo)\n\n[1] 500\n\n\nLet’s limit the sample to those who agreed to our consent form. Locate your consent variable and subset on those who agree.\n\nexpdemo &lt;- subset(expdemo, QID1 == \"I Agree\")\n\nAt this stage, we just have fake/test respondents. However, we can still see if the randomization works properly and if the outcome questions are populating in the way we want.\nFor example, we want about a quarter of respondents assigned to each of the experimental conditions.\n\ntable(expdemo$treat)\n\n\nendorsement   operation     stayout      threat \n         71          58          57          56 \n\n\nAnd let’s make sure our outcomes are populating correctly. Note how people from each condition have populated the outcomes. This gives us confidence that the survey logic is working correctly. If, for example, no one from the endorsement condition had answered the outcome, this might mean we had a typo or other error in our survey logic.\n\ntable(expdemo$approval)\n\n\n              Approve somewhat               Approve strongly \n                            51                             46 \n           Disapprove somewhat            Disapprove strongly \n                            49                             43 \nNeither approve nor disapprove \n                            53 \n\ntable(condition=expdemo$treat,\n      outcome = expdemo$approval)\n\n             outcome\ncondition     Approve somewhat Approve strongly Disapprove somewhat\n  endorsement               13               15                  19\n  operation                 13                9                   9\n  stayout                   15               11                   7\n  threat                    10               11                  14\n             outcome\ncondition     Disapprove strongly Neither approve nor disapprove\n  endorsement                  13                             11\n  operation                     9                             18\n  stayout                      11                             13\n  threat                       10                             11\n\n\nIf our survey programming was all set, at this point, you could actually set up your entire R code and analysis based on the fake data. That would mean that all you have to do after you run the survey with real respondents is switch the dataset you load into the software. That would be the ultimate “pre-analysis plan.”\nOnce you are done testing in Qualtrics, back in the Data and Analysis page, you can delete all responses using Tools -&gt; Delete data.\nOnce you are done testing and revising the survey, you are now ready to integrate it with your preferred survey firm/recruiting platform. The specific steps from here going forward vary across platforms.\n\n\n3.2.6 Additional Bells and Whistles\nQualtrics has a number of other features you can use, including different question types, the ability to randomize the order of response options, features to require/request responses.You can continue to explore these as you develop your own surveys.\n\nTheir help pages are pretty useful. Here is one on question types.\nThis blog post from Crystal Lewis also mentions several important tips and quirks of Qualtrics to consider when building a survey.\n\nFor those familiar with “conjoint experiments” that have a lot of randomization, Anton Strezhnev has developed a tool for programming these in Qualtrics. See information here.\nIt is also possible to download data from Qualtrics directly into R using an R package here."
  },
  {
    "objectID": "04-Uncertainty.html#standard-errors",
    "href": "04-Uncertainty.html#standard-errors",
    "title": "4  Uncertainty",
    "section": "4.1 Standard Errors",
    "text": "4.1 Standard Errors\nStandard errors represent the standard deviation of a sampling distribution.\nWhat is the standard deviation?\nMeasure of spread: typical deviation of an observation from the mean.\n \nFrom the Cartoon Guide to Statistics\nTo calculate the standard deviation:\n\nTake a squared deviation from the mean for a unit \\(i\\). \\[\\begin{align*}\n&= (y_i -  \\bar{y})^2\n\\end{align*}\\]\nDo this for each unit \\(i\\) out of a sample. Take the sum. \\[\\begin{align*}\n&= \\sum_{i=1}^{N} (y_i -  \\bar{y})^2\n\\end{align*}\\]\nDivide over the total sample. When we are dealing with a sample from a population whose mean is unknown (usually the case), we have to take N-1 instead of N. \\[\\begin{align*}\n&= \\frac{1}{N-1} \\sum_{i=1}^{N} (y_i -  \\bar{y})^2\n\\end{align*}\\]\nTake the square root \\[\\begin{align*}\ns &= \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i -  \\bar{y})^2 }\n\\end{align*}\\] Remember: standard deviation is the square root of the variance!\n\nLet’s say these were the data for a sample of 10 voters’ scores on a feeling thermometer of their views toward liberals.\n\nfts &lt;- c(40, 95, 100, 5, 75, 80, 65, 100, 90, 28)\n\n## 1. take the squared deviation from the mean\nsq.dev &lt;- (fts - mean(fts))^2\n\n## 2. Take the sum\nsum.sq.dev &lt;- sum(sq.dev)\n\n## 3. Divide over N - 1\nsum.sq.dev.n &lt;- sum.sq.dev/(length(fts) - 1)\n\n## 4. Take the square root\ns &lt;- sqrt(sum.sq.dev.n)\ns\n\n[1] 33.02457\n\n## optional code\nsd(fts) # or sqrt(var(fts)) \n\n[1] 33.02457\n\n\nWhat is a sampling distribution?\nThe experiment we happen to conduct yields an estimate of the average treatment effect, but in a different randomization, our estimate might have been different.\n\nSampling distribution refers to the set of estimates that could have been generated by every possible random assignment.\nThe standard error is a measure of the spread of this distribution.\nGood news: Under the central limit theorem, this distribution approximates the shape of a normal distribution when there are sufficient observations.\n\nWhy is this good news? Going to help us estimate uncertainty.\n\n\n\n4.1.1 Standard Error of the Mean\nThe population mean and variance are \\(\\mu_y\\) and \\(\\sigma^2_y\\) for some variable \\(Y\\) and \\(\\sigma_y\\) is the standard deviation. We want to know the variability of our sample mean \\(\\bar{Y}\\).\nWell we already know the mean of our sample mean (\\(\\bar{Y}\\)) is the population mean \\((\\mu_y)\\):\n\\[\\begin{align*}\nE(\\bar{Y}) &= E[\\frac{1}{N}\\sum_{i=1}^{N} y_i] \\\\\n&= \\frac{1}{N}*[E(y_1) + E(y_2) + ... + E(y_N)]\\\\\n&= \\frac{1}{N}*N\\mu_y\\\\\n&= \\mu_y\n\\end{align*}\\]\nWhat about the variance of \\(\\bar{Y}\\)? We call the variance of our population mean: \\(\\sigma^2\\). \n\\[\\begin{align*}\nVar(\\bar{Y}) &= Var[\\frac{1}{N}\\sum_{i=1}^{N} y_i] \\\\\n&= \\frac{1}{N^2}*[Var(y_1) + Var(y_2) + ... + Var(y_N)]\\\\\n&= \\frac{1}{N^2}*N\\sigma^2\\\\\n&= \\frac{\\sigma^2}{N}\n\\end{align*}\\]\nThen to get to the standard error, we take the square root:\n\\[\\begin{align*}\n&= \\frac{\\sigma}{\\sqrt{N}}\n\\end{align*}\\]\nWe cannot observe the actual \\(\\sigma\\) so instead, we will follow the practice of using a “sample analogue.” In our case, this is \\(s\\), the sample standard deviation:\n\nSo we have an estimate for the standard error of our sample mean:\n\n\\[\\begin{align*}\n\\widehat{SE}_m &= \\frac{s}{\\sqrt{N}}\\\\\n\\end{align*}\\]\nComputing the estimate of our standard error\n\nTake the standard deviation of our sample. Recall:\n\n\\[\\begin{align*}\ns &= \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i -  \\bar{y})^2 }\\\\\n\\end{align*}\\]\n\nDivide by the square root of the sample size.\n\n\\[\\begin{align*}\n\\widehat{SE}_m &= \\frac{s}{\\sqrt{N}}\\\\\n\\end{align*}\\]\nExample\nWhat’s our estimate for the mean and standard error for feeling thermometer scores toward liberals?\n\n## 1. Take standard deviation\nst.dev.fts &lt;- sd(fts)\n\n## 2. Divide by square root of sample size\nse.fts &lt;- st.dev.fts/sqrt(length(fts))\nse.fts\n\n[1] 10.44329\n\n## Alternative\nsqrt(var(fts)/length(fts))\n\n[1] 10.44329\n\n## What's the mean?\nmean(fts)\n\n[1] 67.8\n\n\n\n\n4.1.2 Standard error for a difference in means\nSo we have an estimate for the standard error of our sample mean: But often we want the standard error of a difference in means, correspondig to the uncertainty of \\(\\widehat{ATE}\\).\nWhen we take the difference in variances from two independent samples, we add their variances:\n\\[\\begin{align*}\n\\widehat{SE}_{d-i-m} &= \\sqrt{\\frac{\\widehat{Var}(Y_i(1))}{m} + \\frac{\\widehat{Var}(Y_i(0))}{N-m}}\\\\\n\\end{align*}\\]\nNote: Let’s inspect this formula. What does it tell us about when the standard error will be larger/smaller?\n\nGives us insight into designs with blocking, matched pairs"
  },
  {
    "objectID": "04-Uncertainty.html#confidence-intervals",
    "href": "04-Uncertainty.html#confidence-intervals",
    "title": "4  Uncertainty",
    "section": "4.2 Confidence Intervals",
    "text": "4.2 Confidence Intervals\n\nTake a sample statistic (e.g.,\\(\\bar{Y}\\))\nSet a test value. A common one is \\(\\alpha = 0.05\\)\nFind the critical value associated with the test level. Example:\n\n\\[\\begin{align*}\nz_{crit (1-\\alpha/2)} &= 1.96\\\\\n\\end{align*}\\]\n\nMultiply the critical value by the standard error of your statistic, and add and subtract from the statistic\n\n\\[\\begin{align*}\nCI &= \\bar{Y} +/- crit.value*\\widehat{SE}_{\\bar{Y}}\\\\\n\\end{align*}\\]\nCareful when interpreting CI’s: note that the interval may vary from experiment to experiment, while the parameter stays fixed.\nExample computing confidence intervals\n\nm.fts &lt;- mean(fts)\n\n## What's our test level? .05\nalpha &lt;- .05\n\n## critical value for normal distribution\ncrit.z &lt;- qnorm(1- alpha/2)\n## critical value for t-distribution\ncrit.t &lt;- qt(1- alpha/2, df = (length(fts)-1))\n\n## Confidence interval using t-distribution\nci.ub &lt;- m.fts + crit.t*se.fts\nci.lb &lt;- m.fts - crit.t*se.fts\nc(ci.lb, ci.ub) \n\n[1] 44.17565 91.42435\n\n## Alternative using the R t.test function\nt.testfts &lt;- t.test(fts)\nt.testfts$conf.int[1:2]\n\n[1] 44.17565 91.42435"
  },
  {
    "objectID": "04-Uncertainty.html#hypothesis-tests",
    "href": "04-Uncertainty.html#hypothesis-tests",
    "title": "4  Uncertainty",
    "section": "4.3 Hypothesis Tests",
    "text": "4.3 Hypothesis Tests\nGenerally, we want to actually test hypotheses. We will use the null hypothesis testing framework. In this framework, we collect evidence to reject or fail to reject a naive starting assumption: the null hypothesis.\nTypical setup for two-sample test.\n\nNull hypotheis: \\(H_o\\): \\(\\mu_{Y(1)} = \\mu_{Y(0)}\\)\nAlternative hypothesis: \\(H_a\\): \\(\\mu_{Y(1)} \\neq \\mu_{Y(0)}\\);\n\nor \\(H_a\\): \\(\\mu_{Y(1)} &gt; \\mu_{Y(0)}\\) or \\(H_a\\): \\(\\mu_{Y(1)} &lt; \\mu_{Y(0)}\\)\n\n\nReview: Let’s say we do a two-sided test and get a p-value from our t-test of 0.003. What should we conclude?\n\nWait, what’s a p-value? How should we interpret this p-value? (pg. 64, Gerber and Green)\n\n\n4.3.1 t-tests\nA common implementation of hypothesis tests for comparing averages of two groups is the t-test.\nSingle population \\[\\begin{align*}\nt &= \\frac{\\bar{X} - \\mu_o}{\\widehat{SE}_m}\\\\\n\\end{align*}\\]\nTwo populations\n\\[\\begin{align*}\nt &= \\frac{(\\bar{X_1} - \\bar{X_0})- (\\mu_1 - \\mu_0)}{\\widehat{SE}_{d-i-m}}\\\\\n\\end{align*}\\]\nIn each case, we standardize our estimates according to the student’s t-distribution. We look to see just how extreme our t statistic is. t is our test statistic, a ratio between the size of the difference in means over the variability in the underlying data, represented by the standard error. Here is a relatively accessible summary of t values.\n\n\n4.3.2 p-values\n\nThe p-value asks: What is the probability of getting a result this extreme or more extreme “by chance”/“if the null were true”? In a world where the null is true, we still might not get a t=0 in every sample. The t-distribution represents the range of t-values we might expect to see with some probability under the assumption the null is true. We need to quantify how likely it would be to get our t-statistic in this world where the null is true.\n\nLower-tailed test, p-value \\(= Pr(T &lt; t | H_o\\) is true)\nUpper-tailed test, p-value \\(= Pr(T &gt; t | H_o\\) is true)\nTwo-sided test is specified by: p-value \\(= 2 * P(T &gt; |t| \\hspace{1mm} | H_o\\) is true)\n\nWe primarily use two-sided tests. To get the p-value, we need the degrees of freedom because the t-distribution varies somewhat in shape according to the degrees of freedom, which are primarily a function of the sample size. Degrees of freedom govern how thick the tails of the distribution are, which can influence and increase the size of the p-value. For one sample tests, it is N-1. For two-sample t-tests, the degrees of freedom calculation can be more complicated.\nIf we use the Welch calculation for unequal variances, which is the default setting in the R t.test function it is: df\\(=\\frac{\\widehat{SE}^4}{ \\frac{\\widehat{Var}(Y_i(1))^2}{m^2(m-1)} + {\\frac{\\widehat{Var}(Y_i(0))^2}{(N-m)^2(N-m-1)}}}\\).\n\nFortunately, the t.test function in R will calculate that for you."
  },
  {
    "objectID": "04-Uncertainty.html#empirical-application",
    "href": "04-Uncertainty.html#empirical-application",
    "title": "4  Uncertainty",
    "section": "4.4 Empirical Application",
    "text": "4.4 Empirical Application\nWe will use data from the following experiment: “Social Exclusion and Political Identity: The Case of Asian American Partisanship” by Alexander Kuo, Neil Malhotra, and Cecilia Mo (2016). The data set based on authors’ replication file here\n\nResearch Question: Do feelings of social exclusion lead Asians to develop more negative feelings toward the Republican Party?\n\nSample: 114: 61 self-reported Asian, 53 self-reported white\nTreatment: Manipulate feelings of social exclusion.\nOutcome: Difference in views toward Democratic vs. Republican Party\n\nClose-mindedness, ignorance, represent interests, likes/dislikes, feeling thermometer, party ID, and the average of these six\n\n\n\nLet’s put this in the potential outcomes framework.\n\nFor a given unit \\(i\\) what are the potential outcomes we are interested in?\nWhat is the \\(\\tau_i\\) we are interested in?\nHow are we going to estimate it?\n\n\n4.4.1 Treatment\nFor those in the treatment group, a white female assistant to the research team says, “I’m sorry; I forgot that this study is only for US citizens. Are you a US citizen? I cannot tell.” If the subject was a US citizen, the assistant was instructed to say “OK, go ahead” and have the respondent start the survey; if the subject was not a US citizen, the assistant was instructed to pause and then say “it’s OK, go ahead.” Subjects then completed an online survey of their political attitudes.\n\nIs this treatment a good treatment? Use the principles we discussed last section to evaluate this implementation.\n\nYour ideas …\n\n\n\n4.4.2 Analysis\nLet’s load the data.\n\nlibrary(foreign)\nexclusion &lt;- read.dta(\"data/exclusion.dta\")\n\n## Explore data\n## How many subjects?\n## How many Asian vs. White subjects\n## What proportion of subjects were treated?\n\n\n## Let's relabel the names to something sensible\nnames(exclusion)\n\n[1] \"v1\"            \"v2\"            \"v3\"            \"v4\"           \n[5] \"v5\"            \"v6\"            \"study2_avg\"    \"treatment_cit\"\n[9] \"asiant\"       \n\n## v1 is difference between dem - rep in closed mindedness\nnames(exclusion)[1] &lt;- \"clmindeddr\"\n## v2 is difference between dem - rep in ignorance\nnames(exclusion)[2] &lt;- \"ingnorantdr\"\n\n## What if you don't want to have to find the number?\nnames(exclusion)[names(exclusion) == \"v3\"] &lt;- \"netlikesdr\"\n\nnames(exclusion)[4] &lt;- \"piddr\" # pid\nnames(exclusion)[5] &lt;- \"ftdr\" # feeling thermometer\nnames(exclusion)[6] &lt;- \"repdr\" # represents interests\n\n\n## Difference in means for the average\n## Overall\nd.i.m &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1]) -  \n               mean(exclusion$study2_avg[exclusion$treatment_cit == 0])\n\n## Among whites\ndiff.whites &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1 &\n                                     exclusion$asiant == 0]) -  \n               mean(exclusion$study2_avg[exclusion$treatment_cit == 0 &\n                                          exclusion$asiant == 0 ])\n## Among asians\ndiff.asians &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1 &\n                                     exclusion$asiant == 1]) -  \n               mean(exclusion$study2_avg[exclusion$treatment_cit == 0 &\n                                          exclusion$asiant == 1 ])\n\nWe could also subset the data by race/ethnicity group. Let’s do that and then calculate our t-test and uncertainty by hand and using the R functions.\n\n## Subset data for only Asian respondents\nasians &lt;- subset(exclusion, asiant == 1)\n\n## t-test by hand for sample of Asian Respondents\n## Calculating Standard error\n\n## Get N for each group\nn.asianst1 &lt;- length(asians$study2_avg[asians$treatment_cit == 1])\nn.asianst0 &lt;- length(asians$study2_avg[asians$treatment_cit == 0])\n\n## Get variance for each group\nv.asianst1 &lt;- var(asians$study2_avg[asians$treatment_cit == 1])\nv.asianst0 &lt;- var(asians$study2_avg[asians$treatment_cit == 0])\n\n## Standard error\nse.diffasians &lt;- sqrt(v.asianst1/n.asianst1 + v.asianst0/n.asianst0)\n\n## t-statistic\nt.diffasians &lt;- diff.asians/se.diffasians\n\n## Degrees of freedom\nt.df &lt;- (se.diffasians)^4/\n  (v.asianst1^2/(n.asianst1^2*(n.asianst1 - 1)) \n   + v.asianst0^2/(n.asianst0^2*(n.asianst0 - 1)))\n\n## p-value for two-sided test\np.asians &lt;- (1- pt(abs(t.diffasians), t.df))*2\n\nWe could visualize this according to the t-distribution with degrees of freedom equal to t.df: 50.73481 and our t-value of 2.196597 in the dashed red line.\n\n\n\n\n\nTo get our p-value in a two-sided test, we compute the area to the right of this and to the left of its corresponding value on the opposite side of the distribution (equivalently due to the symmetric nature of the distribution, we can take 2 \\(\\times\\) either area). This area represents a probability, as the total area under the curve sums to 1.\n\n\n\n\n\nA shortcut for computing the results is to use the R function. When learning a new function, you can access the help files in R by typing ?FUN into the console. Example: t.test.\n\n## t-test the quick way!\nasians.t &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1],\n                   asians$study2_avg[asians$treatment_cit == 0])\n\n\nwhites &lt;- subset(exclusion, asiant == 0)\nwhites.t &lt;- t.test(whites$study2_avg[whites$treatment_cit == 1],\n                   whites$study2_avg[whites$treatment_cit == 0])\n\nWhat are our conclusions about the hypothesis tests?\n\n\n4.4.3 Heterogeneous Treatment Effects\nThe researchers believe that the size of the treatment effects will be different depending on the race/ethnicity of the participant.\n\nShould we compare the treatment effects among Asians vs. whites?\n\nIf we do, can we say that being Asian caused a different reaction to microaggressions than being white?\nOverall, what are the limits of studying heterogeneity?\n\n\nOne approach to detecting a heterogeneous treatment effect is to use an interaction in a linear regression model.\nAs we discussed in the second section, when you have a treatment categorical variable, the regression coefficient \\(\\hat \\beta\\) represents the difference in means.\n\nWhen we interact this treatment indicator with a second variable, it will tell us how much the treatment effect varies according to the levels of that second variable.\n\nLet’s start by calculating our treatment effects with the regression approach.\n\n## Linear regression lm(y ~ x, data = nameofyourdataframe)\nasians.r &lt;- lm(study2_avg ~ treatment_cit, data = asians)\n\n\nsummary(asians.r)\n\n\nwhites.r &lt;- lm(study2_avg ~ treatment_cit, data = whites)\n\n\nsummary(whites.r)\n\nWhy is the p-value slightly different here?\n\n\nTry on your own, then expand for the answer.\n\nWe do not assume our groups have equal variances when we do the t-test, but regression relies on a pooled variance estimator, which differs slightly. We can recover the p-value in two ways.\n\n## Indicate var.equal=T\nasians.t.p &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1],\n                   asians$study2_avg[asians$treatment_cit == 0], var.equal = T)\nasians.t.p$p.value\n\n[1] 0.02951252\n\n## By hand, use pooled estimator for variance/standard error/degrees of freedom\npooled.var &lt;- ((n.asianst1 - 1) * v.asianst1 + \n                 (n.asianst0 - 1)*v.asianst0)/ (n.asianst0 + n.asianst1 -2)\npooled.se &lt;- sqrt(pooled.var) * sqrt( 1/n.asianst0 + 1/n.asianst1)\npooled.t &lt;- diff.asians/pooled.se\npooled.p &lt;- (1- pt(abs(pooled.t), (n.asianst0 + n.asianst1 -2)))*2\npooled.p\n\n[1] 0.02951252\n\n\n\nLet’s now add the interaction term using the asterisk symbol:\n\n## Using an interaction\nhet.r &lt;- lm(study2_avg ~ treatment_cit + asiant \n            + treatment_cit*asiant, data = exclusion)\nsummary(het.r)\n\n\nCall:\nlm(formula = study2_avg ~ treatment_cit + asiant + treatment_cit * \n    asiant, data = exclusion)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52680 -0.07572  0.02925  0.09661  0.32821 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.67267    0.02976  22.606   &lt;2e-16 ***\ntreatment_cit        -0.03200    0.04517  -0.708   0.4802    \nasiant               -0.07654    0.04244  -1.803   0.0741 .  \ntreatment_cit:asiant  0.12517    0.06153   2.034   0.0443 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.163 on 110 degrees of freedom\nMultiple R-squared:  0.04907,   Adjusted R-squared:  0.02314 \nF-statistic: 1.892 on 3 and 110 DF,  p-value: 0.1351\n\n## How do we interpret each coefficient?\n\nWe now have a t-test for the interaction term, specifically. Be mindful that when you include an interaction term in a regression model, it changes the way we interpret the two other “main effects.”\n\n\n4.4.4 Difference in Proportions\nIf we have a proportion as an average outcome instead of a mean, we may wish to adjust how we calculate uncertainty to better reflect the nature of a dichotomous outcome variable.\nWhen we are comparing two proportions, we can use the prop.test function in R, which will adjust this calculation for us.\n\n## Create a proportion variable where 1=dem, 0=rep\ntable(exclusion$piddr)\n\n\n                0 0.200000002980232 0.400000005960464 0.600000023841858 \n                2                 7                12                28 \n0.800000011920929                 1 \n               35                30 \n\nasians$dem &lt;- ifelse(asians$piddr &gt; .5, 1, 0)\n\n## Calculate difference in proportions by hand\nm1.asians &lt;- mean(asians$dem[asians$treatment_cit == 1])\nm0.asians &lt;- mean(asians$dem[asians$treatment_cit == 0])\nm1.asians - m0.asians\n\n[1] 0.1163793\n\n\n\n## Use prop.test: NOTE THE DIFFERENT SYNTAX FROM t.test\n## x is the \"number of successes\" i.e., number of 1's for each group\n## n is sample size for each group\np.test.asians &lt;- prop.test(x = c(sum(asians$dem[asians$treatment_cit == 1]),\n                sum(asians$dem[asians$treatment_cit == 0])), \n                n = c(length(asians$dem[asians$treatment_cit == 1]), \n                  length(asians$dem[asians$treatment_cit == 0])))\n\n\n## Note if you were to run the standard t-test, \n## the difference would be the same but calculation of uncertainty is different\nt.test(asians$dem[asians$treatment_cit == 1], \n       asians$dem[asians$treatment_cit == 0])\n\n\n    Welch Two Sample t-test\n\ndata:  asians$dem[asians$treatment_cit == 1] and asians$dem[asians$treatment_cit == 0]\nt = 1.1599, df = 52.547, p-value = 0.2514\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.08491716  0.31767578\nsample estimates:\nmean of x mean of y \n0.8750000 0.7586207 \n\n\nWhat do you conclude about the test?"
  },
  {
    "objectID": "04-Uncertainty.html#randomization-inference",
    "href": "04-Uncertainty.html#randomization-inference",
    "title": "4  Uncertainty",
    "section": "4.5 Randomization Inference",
    "text": "4.5 Randomization Inference\nNull Hypothesis of No Average Effect vs. Sharp Null Hypothesis of No Effect (pg. 62)\n\n\\(\\mu_{Y(1)} = \\mu_{Y(0)}\\) vs.\n\\(Y_i(1) = Y_i(0)\\) for all i\n\nWhat are the key differences here?\n\nThe treatment has no effect: \\(Y_i(1) = Y_i(0)\\) for all \\(i\\).\nSuppose we are in the world where the sharp null is true.\nLet’s simulate what the sampling distribution under that null distribution looks like.\nWe assess the distribution relative to the ATE we observe under the assignment in our sample\nHow likely is it we would observe our ATE, given the null distribution?\n\nExample Here is our data for 7 observations, where 4 are assigned to treatment, 3 to control.\n\nOur estimate for the average treatment effect is \\(6-3 = 3.\\)\nSuppose the sharp null is true: \\(Y_i(1) = Y_i(0)\\). This means \\(\\tau_i\\) = 0 for all \\(i.\\)\n\nIn our null world, if we know \\(Y_i\\) for each \\(i\\) and \\(\\tau_i\\) for each \\(i\\), we can solve for the missing potential outcome.\n\nFor randomization inference, what we do now is simulate possible randomizations– what if a different set of observations were treated each time?\n\nWhat is our average treatment effect for d? 18/4 - 15/3 = -.5\nRepeat for all (or a lot of) possible permutations of d. This gives us an implied null distribution of the average treatment effect under the sharp null. Note: it won’t always be zero. It will be a distribution around zero. We will compare how extreme our observed estimate of the average treatment effect is compared to this distribution under the null.\nEmpirical example with social exclusion experiment\nAlex Coppock has updated the randomization inference package to ri2 in R. More on this package is available here.\n\n## install.packages(\"ri2\", dependencies=T)\nlibrary(ri2)\n\n## Declare randomization\ndeclaration &lt;- declare_ra(N=nrow(asians), prob=.5)\n\n## Estimate the average treatment effect\nset.seed(1215)\nri2_out &lt;- conduct_ri(\n  formula = study2_avg ~ treatment_cit,\n  assignment = \"treatment_cit\",\n  declaration = declaration,\n  sharp_hypothesis = 0,\n  data = asians\n)\n\nPlot and compare distribution to observed ATE\n\nplot(ri2_out)\n\n\n\n\nSummarize Output\n\nsummary(ri2_out)\n\n           term   estimate two_tailed_p_value\n1 treatment_cit 0.09317187              0.033\n\n\nWe can manually see what the package is doing by counting how many of the simulated estimates from the empirical distribution of the sharp null hypothesis were more extreme than our estimate from the study. Note that in this case, our p-value is very similar to the t-test.\n\nestimate &lt;- tidy(ri2_out)$estimate\nnsims &lt;- length(ri2_out$sims_df$est_sim)\nsimstimates &lt;- ri2_out$sims_df$est_sim\n## Two-tailed p-value\nlength(simstimates[abs(simstimates) &gt;= abs(estimate)])/nsims\n\n[1] 0.033"
  },
  {
    "objectID": "04-Uncertainty.html#a-note-on-expectations-and-variance",
    "href": "04-Uncertainty.html#a-note-on-expectations-and-variance",
    "title": "4  Uncertainty",
    "section": "4.6 A Note on Expectations and Variance",
    "text": "4.6 A Note on Expectations and Variance\nThis section includes definitions of expectation and variance and walks through more deliberately how we derive what the variance of a mean is (which we need for standard errors!).\n\nThis video walks you through the steps in the slides from Week 3 Uncertainty with very similar notation. Below, we go through a more elongated version of the derivation that starts from the basic definition of the variance of a random variable.\n\nWe are going to exploit a few “rules” of expectations and variance as we go through the derivation.\n\nThe expectation of a sum is the sum of the expectations: \\(E(X + Y) = E(X) + E(Y)\\)\nThe expectation of a constant (\\(a\\)) is the constant.The expectation of a constant multiplied by a random variable is \\(E(aX) = aE(X)\\)\nWhen our observations are independent (which we are generally assuming), the variance of the sum is equal to the sum of the variance: \\(Var(\\sum_{i=1}^N X_i) = \\sum_{i=1}^N Var(X_i)\\)\nWhen we pull a constant outside the variance operator, we square it: \\(Var(a * X) = a^2Var(X)\\)\n\nThe steps below for deriving the standard error show why this squaring happens.\n\nThe variance of a random variable is: \\(Var(X) = E[(X - \\mu)^2]\\) where \\(\\mu\\) refers to the expected value or ``population mean” of the random variable (i.e., \\(E[(X - \\mu)^2] = E[(X - E(X))^2]\\)).\n\nExpectation and Variance of a Random Variable\nThe expected value of a random variable (e.g., \\(X\\)) is the average value random variable weighted by its probability of ocurrence. We write it as \\(E(X)\\) or sometimes \\(\\mu_x\\).\nThe variance of a random variable is a measure of spread (written as \\(Var(X)\\) or \\(\\sigma_x^2\\)): the degree to which values of the random variable differ from its expected value.\n\nThe square root of the variance is the standard deviation, sometimes written as \\(\\sigma_x\\), a measure of spread describing the typical deviation from the expected value.\n\nOK: The variance of a random variable is defined as the expected squared deviation from the expected value. Let’s do this for a random variable \\(X_i\\) (i.e., from \\(E(X_i)\\) or \\(\\mu\\)):\n\\[\\begin{align*}\nVar(X_i) &= E[(X_i - \\mu)^2]\n%\\\\\n%&= E[X_i^2 - 2 X_i \\mu + \\mu^2]&& \\text{ 1) foil the squared difference}\\\\\n%&=E(X_i^2) - 2* E(X_i)*E(X_i) + [E(X_i)]^2 && \\text{ 2) Move expectation inside, rewrite $\\mu$ as $E%(X_i)$}\\\\\n%&= E(X_i^2) - [E(X_i)]^2 && \\text{ 3) Subtract like terms}\n\\end{align*}\\]\nThe square root of this quantity is the standard deviation.\nExpectation and Variance of a Mean\nWe now consider our mean (e.g., \\(\\bar{X}\\)) as our random variable and will derive its variance. Why?\n\nBecause this gives us the variability in our sampling distribution (how our mean varies) and will get us to our standard error.\nRecall the standard error is simply the standard deviation of our sampling distribution i.e., the square root of the variance of our sample mean as we think about how the mean varies over all possible randomizations.\n\nRecall: The expected value of our sample mean (\\(E(\\bar{X})\\)) can be written as:\n\\[\\begin{align*}\nE(\\bar{X}) &= \\frac{1}{N} \\sum_{i=1}^N E(X_i)\\\\\n\\end{align*}\\]\nSo we now start with \\(Var(\\bar{X})\\) instead of \\(Var(X_i)\\). However, our variance is still defined as a squared deviation. This time it is the squared deviation of a sample mean from the expected value of the sample mean. \\[\\begin{align*}\nVar(\\bar{X}) &= E[(\\bar{X} - E(\\bar{X}))^2]\\\\\n&= E[(\\frac{1}{N} \\sum_{i=1}^N X_i - E(\\frac{1}{N} \\sum_{i=1}^N X_i))^2] && \\text{rewrite $\\bar{X}$}\\\\\n&= E[(\\frac{1}{N} [\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i)])^2] && \\text{pull out $\\frac{1}{N}$}\\\\\n&= E[\\frac{1}{N^2}(\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i))^2] && \\text{Note: the squaring of constant}\\\\\n&= \\frac{1}{N^2}E[(\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i))^2]&& \\text{Move constant outside expectation}\\\\\n&= \\frac{1}{N^2} Var(\\sum_{i=1}^N X_i) && \\text{Rewrite as variance of the sum  of $X_i$}\\\\\n&=  \\frac{1}{N^2}\\sum_{i=1}^N Var(X_i) && \\text{Apply rule on variance of sum}\\\\\n&=  \\frac{1}{N^2}*(Var(X_1) + Var(X_2) +...+ Var(X_N)) && \\text{Write out the sum}\\\\\n&=  \\frac{1}{N^2}*N* \\sigma^2 && \\text{Substitute our known $\\sigma^2$ from above}\\\\\n&= \\frac{\\sigma^2}{N}\n\\end{align*}\\]\nNote: the square root of this is our standard deviation aka our standard error: \\(\\frac{\\sigma}{\\sqrt{N}}\\). We cannot observe \\(\\sigma\\), so we estimate this using the sample standard deviation \\(s\\). This will be useful, for example, if we want a standard error for the mean outcome in our treatment group.\nVariance of our Difference in Means\nNow we last want to quantify the variability in the sampling distribution for our difference in means (\\(\\bar{X}_1 - \\bar{X}_0\\)) assuming our samples and observations are independent. The idea is that every possible randomization similarly generates a different estimate for the difference just as it does for any individual mean.\nWell we just showed we know the variance of each mean separately:\n\n\\(Var(\\bar{X_1}) = \\frac{\\sigma_1^2}{N_1}\\)\n\\(Var(\\bar{X_0}) = \\frac{\\sigma_0^2}{N_0}\\)\n\nNow we have to get the variance of our difference: \\(Var(\\bar{X}_1 - \\bar{X}_0)\\).\nTo do this, we exploit yet another rule for independent samples: that the variance of a difference is equal to the sum of the variances:\n\n\\(Var(\\bar{X}_1 - \\bar{X}_0) = Var(\\bar{X}_1) + Var(\\bar{X}_0)\\)\nThe standard deviation is again the square root of this: \\(\\sqrt{Var(\\bar{X}_1) + Var(\\bar{X}_0)}\\)\n\nOk writing this out: \\[\\begin{align*}\nVar(\\bar{X}_1 - \\bar{X}_0) &= Var(\\bar{X}_1) + Var(\\bar{X}_0)\\\\\n&= \\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}\n\\end{align*}\\]\nNow if we want the standard error we get: \\[\\begin{align*}\n\\sqrt{Var(\\bar{X}_1 - \\bar{X}_0)} &= \\sqrt{Var(\\bar{X}_1) + Var(\\bar{X}_0)}\\\\\n&= \\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}\n\\end{align*}\\]\nLike before, we use sample substitutes (\\(s_1\\) and \\(s_0\\)) where \\(m\\) represents the number of units in the treatment and \\(N-m\\), the number of unites in the control (switching notation here to match the book):\n\\[\\begin{align*}\n\\widehat{SE}_{d-i-m} &= \\sqrt{\\frac{s^2_1}{m} + \\frac{s^2_0}{N-m}}\n\\end{align*}\\]\nNote: this estimate of the standard error is considered conservative and is only appropriate when samples are independent. We will discuss alternative measures of variance.\n\nFor example, when we use OLS to get out difference in means, we will use a slightly different “pooled variance estimator” where we assume \\(\\sigma_1^2 = \\sigma_0^2\\).\n\nHere, the pooled sample variance is: \\(s_{pooled}^2 = \\frac{(n_1 -1)s^2_1 + (n_0-1)s^2_0}{(n_1 +n_0 -2)}\\)\nwhere the standard error is: \\(\\sqrt{s_{pooled}^2} * \\sqrt{(\\frac{1}{n_1}+\\frac{1}{n_0})}\\)"
  },
  {
    "objectID": "04-Uncertainty.html#equivalence-of-t-test-and-anova",
    "href": "04-Uncertainty.html#equivalence-of-t-test-and-anova",
    "title": "4  Uncertainty",
    "section": "4.7 Equivalence of t-test and ANOVA",
    "text": "4.7 Equivalence of t-test and ANOVA\nYou may be familiar with ANOVA as a way to test for group differences. When you have two groups, the t-test and ANOVA are actually equivalent. Let’s convince ourselves of this.\n\n## 1) t-test where variances are assumed to be equal (often we leave this unequal, which adjusts for unequal variances)\nt1 &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1],\n                   asians$study2_avg[asians$treatment_cit == 0],\n                   var.equal=T)\n\n## 2) One-way anova\n## create variable that is back out vs. stay out only\na1 &lt;- aov(study2_avg ~ treatment_cit, data = asians)\na1.sum &lt;- summary(a1)\n\n## In one-way ANOVA test, a significant p-value indicates that\n## at least one of the group means are different, but we don’t know which \n## pairs of groups are different.\n## However, if we only have two groups, ANOVA reduces to a t-test\n\n## 3) Also equivalent to simple regression\nl1 &lt;- lm(study2_avg ~ treatment_cit, data = asians)\n\n\n## Prove to yourself the equivalence\n## 1) Compare p-values of the treatment effect in each case\nt1$p.value\n\n[1] 0.02951252\n\na1.sum[[1]]$`Pr(&gt;F)`\n\n[1] 0.02951252         NA\n\nsummary(l1)$coefficients[2, 4]\n\n[1] 0.02951252\n\n## 2) Compare test-statistics\n## Note it's the same F-statistic in ANOVA and lm\nsummary(l1)$f\n\n    value     numdf     dendf \n 4.976264  1.000000 59.000000 \n\na1.sum[[1]]$`F value`\n\n[1] 4.976264       NA\n\n## And woohoo! the f-statistic is actually equivalent to our t-stat^2\n## (with two groups, sqrt(f) equals the absolute value of t)\nt1$statistic^2\n\n       t \n4.976264 \n\n\nANOVA and t-test diverge in estimates when you have more than 2 groups\n\nANOVA tests if you have at least one sig. diff (a “joint test” of statistical sig)\nt-tests are meant for pairwise comparisons of significance\nBUT there are “post-hoc” anova tests to look at pairwise comparisons,\nAs there are multiple-testing corrections for t-tests\n\nMore on multiple testing adjustments"
  },
  {
    "objectID": "04-Uncertainty.html#comparing-different-types-of-tests",
    "href": "04-Uncertainty.html#comparing-different-types-of-tests",
    "title": "4  Uncertainty",
    "section": "4.8 Comparing different types of tests",
    "text": "4.8 Comparing different types of tests\nWe have suggested that using a linear regression or a t-test result in the same difference-in-mean estimates with estimating average treatment effects.\nThere may be some slight differences in the uncertainty depending on how you specifiy the t-test.\nA t-test generally uses this formula for standard errors which allows the variances to be unequal between the two groups var.equal = F. However, this is not the only way to estimate uncertainty when comparing two independent groups.\n\\[\\begin{align*}\n\\widehat{SE}_{d-i-m} &= \\sqrt{\\frac{s^2_1}{m} + \\frac{s^2_0}{N-m}}\n\\end{align*}\\]\n\nasians.t &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1],\n                   asians$study2_avg[asians$treatment_cit == 0],\n                   var.equal=F)\nasians.t$stderr\n\n[1] 0.04241645\n\n## by hand\n## Get N for each group\nn.asianst1 &lt;- length(asians$study2_avg[asians$treatment_cit == 1])\nn.asianst0 &lt;- length(asians$study2_avg[asians$treatment_cit == 0])\n\n## Get variance for each group\nv.asianst1 &lt;- var(asians$study2_avg[asians$treatment_cit == 1])\nv.asianst0 &lt;- var(asians$study2_avg[asians$treatment_cit == 0])\n\n## Standard error\nse.diffasians &lt;- sqrt(v.asianst1/n.asianst1 + v.asianst0/n.asianst0)\nse.diffasians\n\n[1] 0.04241645\n\n\nIt is also common to use an assumption that the variances between groups are equal var.equal = T, in which case the variance is calculated using a “pooled” estimator.\n\\[\\begin{align*}\n\\widehat{SE}_{d-i-m pooled} &= \\sqrt{\\frac{(m-1)s^2_1 + (N -m-1)s^2_2}{N - 2}}*\\sqrt{\\frac{1}{m} + \\frac{1}{N-m}}\n\\end{align*}\\]\n\nasians.t &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1],\n                   asians$study2_avg[asians$treatment_cit == 0],\n                   var.equal=T)\nasians.t$std.err\n\nNULL\n\n## By hand, use pooled estimator for variance/standard error/degrees of freedom\npooled.var &lt;- ((n.asianst1 - 1) * v.asianst1 + \n                 (n.asianst0 - 1)*v.asianst0)/ (n.asianst0 + n.asianst1 -2)\npooled.se &lt;- sqrt(pooled.var) * sqrt( 1/n.asianst0 + 1/n.asianst1)\npooled.se\n\n[1] 0.04176698\n\n\nThe pooled variance t-test and simple linear regression procedure will result in the same estimates of uncertainty.\n\\[\\begin{align*}\n\\widehat{SE}_{\\hat \\beta } &= \\sqrt{\\frac{1}{N-2} \\times \\frac{\\sum (y_i - \\hat y_i)^2}{\\sum (x_i - \\bar x)^2}}\n\\end{align*}\\]\n\nasians.fit &lt;- lm(study2_avg ~ treatment_cit, data=asians)\n\n## Manual application of formula\ny &lt;- asians$study2_avg\nyhat &lt;- fitted(asians.fit)\nx &lt;- asians$treatment_cit\nxbar &lt;- mean(asians$treatment_cit)\nN &lt;- n.asianst0 + n.asianst1\n\nSSy &lt;- sum((y - yhat)^2)\nSSx &lt;- sum((x - xbar)^2)\n\nreg.SE &lt;- sqrt(1/(N-2) * (SSy/SSx))\nreg.SE\n\n[1] 0.04176698\n\n## compare to standard error extracted from model summary\nsummary(asians.fit)$coefficients[2,2]\n\n[1] 0.04176698\n\n\nThis is also equivalent to the standard error of the estimate in an ANOVA comparison with two groups, as ANOVA has an equivalence to regression and the t-test in the case of a two-group comparison.\n\na1 &lt;- aov(study2_avg ~ treatment_cit, data = asians)\n\nsqrt(diag(vcov(a1)))[2]\n\ntreatment_cit \n   0.04176698 \n\n\nIn practice, researchers use each of these methods, even though there may be minor differences."
  },
  {
    "objectID": "04-Uncertainty.html#experimental-design-complications",
    "href": "04-Uncertainty.html#experimental-design-complications",
    "title": "4  Uncertainty",
    "section": "4.9 Experimental Design Complications",
    "text": "4.9 Experimental Design Complications\nThe uncertainty calculations we have done so far have focused on comparisons between two experimental groups, using independent samples (where treatment has been randomly assigned), in a one-shot study.\nOf course, as we read in section 3, these are not the only experimental designs.\n\nWe may have designs with more than two experimental groups\nWe may have designs where we measure an outcome both pre-treatment and post-treatment\nWe may have designs where we expose a subject to multiple experimental treatments (e.g., in conjoint studies).\n\nEach of these cases may present a slight modifications in how we conduct hypothesis tests and compute uncertainty. We will cover these as we encounter them."
  },
  {
    "objectID": "05-Visualization.html#plotting-average-treatment-effects",
    "href": "05-Visualization.html#plotting-average-treatment-effects",
    "title": "5  Visualization",
    "section": "5.1 Plotting Average Treatment Effects",
    "text": "5.1 Plotting Average Treatment Effects\nThe example we will use is from “Black Politics: How Anger Infuences the Political Actions Blacks Pursue to Reduce Racial Inequality” by Antoine J. Banks, Ismail K. White, and Brian D. McKenzie, published in Political Behavior in 2019.\nWe will replicate the results from Study 2, which is a survey experiment. The sample includes 444 Black treated Black respondents recruited by Qualtrics. The excerpt below shows the experimental manipulation.\n\nHere is a short video walking through the code to plot the ATEs using plot and ggplot. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)\n\nLet’s load the data. Note: This file is in a .dta format, but if you try to use read.dta to load it, you may receive an error because it is too new of a Stata format. As an alternative, we can use the rio package to open the file. Install the package, open the package with library an load the data. The rio packages uses a single import function to load data.\n\n## install.packages(\"rio\", dependencies=T)\nlibrary(rio)\nbanks &lt;- import(\"data/banksstudy2.dta\")\n\nThe authors have a variable in their data baddata they use to exclude subjects who failed to follow the instructions of their manipulation. They limit their analyses to those who passed this check. Let’s do the same by removing any subjects that have non-missing values on this variable.\n\nbanks &lt;- subset(banks, is.na(baddata)==T)\n\nLet’s replicate a portion of the analysis presented in Table 3 of the paper.\n\nWe will first calculate our estimate of \\(E(Y_i(1_{anger}) - Y_i(0_{no anger}))\\) using the difference-in-means estimator: \\(\\sum_{i=1}^m Y_i(1_{anger}) - \\sum_{m+1}^{N-m}Y_i(0_{no anger})\\).\n\nWe will compare those in the Anger and Control conditions on the outcome for donations to Black organizations.\nWe will use a t-test to do so.\n\n\nd.i.m &lt;- mean(banks$blackdon[banks$angvcon == 1], na.rm=T) -\n  mean(banks$blackdon[banks$angvcon == 0], na.rm=T)\nt.results &lt;- t.test(banks$blackdon[banks$angvcon == 1],\n                    banks$blackdon[banks$angvcon == 0])\nci &lt;- t.results$conf.int\n\nLet’s repeat for the hope condition.\n\nd.i.m2 &lt;- mean(banks$blackdon[banks$hopevcon == 1], na.rm=T) -\n  mean(banks$blackdon[banks$hopevcon == 0], na.rm=T)\nt.results2 &lt;- t.test(banks$blackdon[banks$hopevcon == 1],\n                    banks$blackdon[banks$hopevcon == 0])\nci2 &lt;- t.results2$conf.int\n\nWe could have alternatively used a linear regression to assess significance or randomization inference.\n\n\nExpand for a randomization inference example.\n\nLet’s focus on just the Anger vs. Control first.\n\nangercontrol &lt;- subset(banks, angvcon == 0 | angvcon ==1)\n\n## remove missing data\nangercontrol &lt;- subset(angercontrol, is.na(blackdon) ==F)\n\n## install.packages(\"ri2\", dependencies=T)\nlibrary(ri2)\n\n## Declare randomization\ndeclaration &lt;- declare_ra(N=nrow(angercontrol), prob=.5)\n\n## Estimate the average treatment effect\nset.seed(1215)\nri2_out &lt;- conduct_ri(\n  formula = blackdon ~ angvcon,\n  assignment = \"angvcon\",\n  declaration = declaration,\n  sharp_hypothesis = 0,\n  data = angercontrol\n)\n\nPlot and compare distribution to observed ATE\n\nplot(ri2_out)\n\n\n\n\nSummarize Output\n\nsummary(ri2_out)\n\n     term  estimate two_tailed_p_value\n1 angvcon 0.9793778              0.041\n\n\nWe can manually see what the package is doing by counting how many of the simulated estimates from the empirical distribution of the sharp null hypothesis were more extreme than our estimate from the study. Note that in this case, our p-value is very similar to the t-test.\n\nestimate &lt;- tidy(ri2_out)$estimate\nnsims &lt;- length(ri2_out$sims_df$est_sim)\nsimstimates &lt;- ri2_out$sims_df$est_sim\n## Two-tailed p-value\nlength(simstimates[abs(simstimates) &gt;= abs(estimate)])/nsims\n\n[1] 0.041\n\n\nWe can compare this to the p-value through the t-test where we assume a t distribution and calculate the area at the extremes as larger or larger than our t-statistic.\n\n\n\n\n\n\n\n5.1.1 ATE using plot\nWhen we want to visualize results in R, generally we plot the main Quantity of Interest\n\nUsually the estimated average treatment effect and/or average outcome from each condition condition\n\nWith uncertainty estimates\nPotentially also showing the distribution of underlying data\n\nSome marker to show a relative benchmark (e.g., a line at 0)\n\nThe plot function in R is based on a coordinate system. We supply the x= and y= values where we want to place points.\n\n\n\n\n\nWe will make a plot to display the two ATE estimates we just calculated.\n\nWe need to supply the exact same number of values for the x-axis as the y-axis. Let’s plot the ATE estimates at points 1 and 2 on the x-axis and at the corresponding y-values for the ATEs we estimated.\n\n\n## Plot \nplot(x = c(1, 2), \n     y = c(d.i.m, d.i.m2))\n\n\n\n\nThis has created the plot, but it is not very informative.\n\nLet’ set the axis dimensions with xlim= an ylim=\nLet’s add a title with main=\n\nWe can adjust the size of the title text with cex.main\n\nLet’s add a label for y and x axis with ylab and xlab\n\nWe can adjust the size of the labels cex.lab\n\n\n\nplot(x = c(1, 2), \n     y = c(d.i.m, d.i.m2),\n     xlim=c(.5, 2.5),\n     ylim = c(-1, 2),\n     main=\"Average Treatment Effects on Donations to Black Organizations\",\n     cex.main=.8,\n     ylab=\"Difference in Donation Amount\",\n     xlab= \"Treatment Comparison\",\n     cex.lab=.8)\n\n\n\n\nIn our case, the values on the x-axis are meaningless. We arbitrarily placed the points at 1 and 2. Let’s get rid of the current x-axis and instead replace it with an axis that labels our comparisons.\n\nWe get rid of the current x-axis with xaxt=\"n\"\nWe create a new axis using the axis function. Note: this function goes below the plot() function instead of inside it.\n\n\nplot(x = c(1, 2), \n     y = c(d.i.m, d.i.m2),\n     xlim=c(.5, 2.5),\n     ylim = c(-1, 2),\n     main=\"Average Treatment Effects on Donations to Black Organizations\",\n     cex.main=.8,\n     ylab=\"Difference in Donation Amount\",\n     xlab= \"Treatment Comparison\",\n     cex.lab=.8,\n     xaxt=\"n\")\naxis(1, at=1:2, labels=c(\"Anger vs. \\n Control\",\"Hope vs. \\n Control\"),\n     tick=F)\n\n\n\n\nWe now have an informative plot of our ATE quantities of interest. However, we still need to add something to visualize uncertainty and a benchmark to indicate the size and/or significance of our quantities.\n\nWe can add a horizonatal line to the plot with abline(h=). Like axis(), this function goes below the plot() function.\nWe can add confidence intervals as vertical line segments to our plot using the lines function. Again, this adds a layer below our plot.\n\n\nplot(x = c(1, 2), \n     y = c(d.i.m, d.i.m2),\n     xlim=c(.5, 2.5),\n     ylim = c(-1, 2),\n     main=\"Average Treatment Effects on Donations to Black Organizations\",\n     cex.main=.8,\n     ylab=\"Difference in Donation Amount\",\n     xlab= \"Treatment Comparison\",\n     cex.lab=.8,\n     xaxt=\"n\")\naxis(1, at=1:2, labels=c(\"Anger vs. \\n Control\",\"Hope vs. \\n Control\"),\n     tick=F)\nabline(h=0, col=\"red3\", lty=2)\nlines(c(1,1), ci)\nlines(c(2,2), ci2)\n\n\n\n\n\n\n5.1.2 ATE with ggplot\nThe package ggplot2 also offers a system of plotting in R. The “gg” in ggplot2 stands for the “Grammar of Graphics.” This program provides another framework for creating figures in R. According to Hadley Wickham, “ggplot2 provides beautiful, hassle-free plots that take care of fiddly details like drawing legends.”\nPractically speaking, ggplot() is another tool to plot the same types of figures we have been making in class. Some people prefer ggplot2 because they find the logic of building figures more intuitive using this framework and/or more aesthetically pleasing. However, both ggplot() and the plots we have been making in class can accomplish the same ultimate goals of data visualization– to communicate information transparently, quickly, accurately, simply, and beautifully. Which types of plots you may prefer is up to your own taste.\nThe syntax for this is different. One of the primary differences is that the ggplot function generally requires that you start from a data.frame object. This means that we will have to organize the set of results we want to plot into a rectangular data.frame.\n\n## Put each result in a vector\nangerresults &lt;- c(d.i.m, ci)\nhoperesults &lt;- c(d.i.m2, ci2)\n\n## Bind these together as rows, store as dataframe\ncomb &lt;- data.frame(rbind(angerresults, hoperesults))\n\n## Give columns informative labels\nnames(comb) &lt;- c(\"ATE\", \"lower\", \"upper\")\n\n## Add group indicator\ncomb$Comparison &lt;- c(\"Anger vs. \\n Control\",\"Hope vs. \\n Control\")\n\nNow we can use the ggplot function from the ggplot2 package. The main plotting function in ggplot2 is the ggplot() function. It will give you access to barplots, boxplots, scatterplots, histograms, etc.\nThe three primary components of a ggplot() are a dataframe (data =), a set of mapping aesthetics (aes()), and geoms (e.g., geom boxplot, geom bar, geom point, geom line, etc.).\n\nThe function ggplot() first takes a dataframe that includes the values you would like to plot (e.g., data = comb).\nThe aesthetics then include the variable names that you want to plot on the x and y axis (e.g., aes(x=Comparison, y=ATE))\n\nAdditional mapping aesthetics can be specified. For example, a third variable (or a repeat of a previous variable) can also be specified (e.g., fill =, colour =, shape =), which acts as a grouping variable. If this is specified, ggplot() will create a corresponding legend for the plot and will color/make different shapes for different groups within this third variable.\n\nAfter closing out the first ggplot() parentheses, you then annotate the plot by adding (+) a geometric layer.\n\nIn the example below, we use the geom_point layer to add the ATEs and geom_errorbar layer to add confidence intervals.\n\n\nThere are many more possibilities for plotting with ggplot(). For additional resources on all that is gg, I recommend the R Graphics Cookbook.\n\nlibrary(ggplot2)\nggplot(comb, aes(x=Comparison, y=ATE))+\n  geom_point()+\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=.1)+\n  theme_bw()\n\n\n\n\nJust like with the other plotting functions in R, you can also specify a number of other arguments to make your plot more informative and aesthetically pleasing. Here, you do this by adding (+) additional arguments. See examples below (e.g., ggtitle, xlab, ylab for titles, ylim for y-axis limits, etc.). We can also add a horizontal line with geom_hline.\n\nggplot(comb, aes(x=Comparison, y=ATE))+\n  geom_point()+\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=.1)+\n  theme_bw()+\n  geom_hline(aes(yintercept=0), linetype=\"dashed\", colour=\"red3\")+\n  ggtitle(\"Average Treatment Effects on Donations to Black Organizations\")+\n  theme(plot.title = element_text(hjust = 0.5))# centers title"
  },
  {
    "objectID": "05-Visualization.html#heterogeneous-treatment-effects",
    "href": "05-Visualization.html#heterogeneous-treatment-effects",
    "title": "5  Visualization",
    "section": "5.2 Heterogeneous Treatment Effects",
    "text": "5.2 Heterogeneous Treatment Effects\nLet’s replicate Figure 3 of the paper to study heterogeneous treatment effects. The authors compute these using a regression analysis. We will focus on the Anger vs. Control condition.\n\nFirst, let’s limit the sample to just these two conditions.\n\nangcontrol &lt;- subset(banks, angvcon == 1 | angvcon == 0)\n\nWe will look at how the effect of anger varies across the Community Nationalism Scale in the variable blackauto3. This is a three-point scale with points at 0,1, and 2. We could treat this as a numeric variable or as a categorical variable. We will first do it as a categorical variable.\n\n## option 1- categorical\nfit &lt;- lm(blackdon ~ angvcon*factor(blackauto3), data=angcontrol)\nsummary(fit)\n\n\nCall:\nlm(formula = blackdon ~ angvcon * factor(blackauto3), data = angcontrol)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8611 -3.1750 -0.1909  3.3953  7.1579 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   2.8421     0.6330   4.490 1.07e-05 ***\nangvcon                       0.3329     0.8840   0.377    0.707    \nfactor(blackauto3)1           1.0051     0.9076   1.107    0.269    \nfactor(blackauto3)2           1.3488     0.8231   1.639    0.103    \nangvcon:factor(blackauto3)1   0.4245     1.2484   0.340    0.734    \nangvcon:factor(blackauto3)2   1.3373     1.1577   1.155    0.249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.902 on 260 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.06442,   Adjusted R-squared:  0.04643 \nF-statistic: 3.581 on 5 and 260 DF,  p-value: 0.003779\n\n\nFocus on the interaction term when interpreting the results for the heterogeneous treatment effects.\n\n## option 2- numeric\nfit.numeric &lt;- lm(blackdon ~ angvcon*blackauto3, data=angcontrol)\n\nWe can then calculate the average treatment effects within each level of blackauto3 using the marginaleffects package.\n\nThe first input is the object name for the regression model (e.g., fit).\nThe variables input is then the treatment condition, or the variable for which you want to estimate the marginal effect on the outcome.\nWe then specify that we want to calculate this contrast across different levels of another variable, “blackauto3”.\n\n\nlibrary(marginaleffects)\noutp &lt;- avg_comparisons(fit, variables = list(angvcon = c(0, 1)),\n                        by = \"blackauto3\",\n                        newdata = datagridcf(blackauto3 = c(0, 1, 2),\n                                             angvcon = c(0,1)))\n\nsummary(outp)\n\n\n    Term          Contrast blackauto3 Estimate Std. Error     z Pr(&gt;|z|)   S\n angvcon mean(1) - mean(0)          0    0.333      0.884 0.377   0.7065 0.5\n angvcon mean(1) - mean(0)          1    0.757      0.882 0.859   0.3902 1.4\n angvcon mean(1) - mean(0)          2    1.670      0.748 2.234   0.0255 5.3\n  2.5 % 97.5 %\n -1.400   2.07\n -0.970   2.49\n  0.205   3.14\n\nColumns: term, contrast, blackauto3, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nThe summary is already in a nice dataframe format, which makes it easy to use ggplot.\n\nggplot(outp, aes(x=blackauto3, y=estimate))+\n  geom_point()+\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1)+\n  theme_bw()+\n  geom_hline(aes(yintercept=0), linetype=\"dashed\", colour=\"red3\")+\n  ggtitle(\"Average Treatment Effects on Donations to Black Organizations \\n by Community nationalism\")+\n  ylab(\"Average Treatment Effects on Black Org. Donations\")+\n  xlab(\"Community Nationalism\")+\n  scale_x_discrete(labels = c(\"Low\", \"Medium\", \"High\"))+\n  theme(plot.title = element_text(hjust = 0.5))# centers title\n\n\n\n\nHere’s an alternative way to look at it with geom_line and geom_ribbon\n\nggplot(outp, aes(x=blackauto3, y=estimate))+\n  geom_point()+\n  geom_line()+\n  geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.4)+\n  theme_bw()+\n  geom_hline(aes(yintercept=0), linetype=\"dashed\", colour=\"red3\")+\n  ggtitle(\"Average Treatment Effects on Donations to Black Organizations \\n by Community nationalism\")+\n  ylab(\"Average Treatment Effects on Black Org. Donations\")+\n  xlab(\"Community Nationalism\")+\n  scale_x_discrete(labels = c(\"Low\", \"Medium\", \"High\"))+\n  theme(plot.title = element_text(hjust = 0.5))# centers title\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\nAn alternative way to represent heterogeneity is instead of plotting the conditional average treatment effects, we can plot the raw outcomes in each condition.\nWe can then calculate the estimated outcomes using the avg_predictions function.\n\nlibrary(marginaleffects)\noutp2 &lt;- avg_predictions(fit, variables = c(\"blackauto3\", \"angvcon\"),\n                         newdata=datagridcf(blackauto3 = c(0, 1, 2),\n                                  angvcon = c(0,1)))\noutp2$blackauto3 &lt;- as.numeric(as.character(outp2$blackauto3))\noutp2$angvcon &lt;- as.factor(outp2$angvcon)\n\n\nggplot(outp2, aes(x=blackauto3, y=estimate,\n                     fill=as.factor(angvcon)))+\n  geom_point()+\n  geom_line()+\n  geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.4)+\n  theme_bw()+\n  ggtitle(\"Average Donations to Black Organizations \\n by Community nationalism\")+\n  ylab(\"Average Black Org. Donations\")+\n  xlab(\"Community Nationalism\")+\n  scale_fill_manual(\"Condition\", labels=c(\"Control\", \"Anger\"), values=c(\"orange\", \"dodgerblue\"))+\n  scale_x_continuous(breaks = c(0,1,2), \n                     labels = c(\"Low\", \"Medium\", \"High\"))+\n  theme(plot.title = element_text(hjust = 0.5))# centers title"
  },
  {
    "objectID": "05-Visualization.html#some-additional-plotting-options",
    "href": "05-Visualization.html#some-additional-plotting-options",
    "title": "5  Visualization",
    "section": "5.3 Some additional plotting options",
    "text": "5.3 Some additional plotting options\nThe common visualizations used to show average treatment effects do not give much information about the distributions of underlying data. Here are a few examples of plotting the underlying distributions.\n\nYou might also explore geom_bar for outcomes that are binary or categorical in nature as an alternative to geom_histogram.\n\nCreate a variable that summarizes all three experimental conditions. This will make it easier to plot data grouped by each condition.\n\nbanks$condition &lt;- NA\nbanks$condition[banks$angvcon == 1] &lt;- \"Anger\"\nbanks$condition[banks$hopevcon == 1] &lt;- \"Hope\"\nbanks$condition[banks$angvcon ==  0 & banks$hopevcon == 0] &lt;- \"Control\"\nbanks$condition &lt;- factor(banks$condition, levels=c(\"Control\", \"Anger\", \"Hope\"))\n\nWe filter out respondents who were not assigned to any condition. You can do this as part of the plot code, or you can banks &lt;- subset(banks, is.na(condition)==F) prior to running the plot code.\n\nlibrary(tidyverse)\nbanks %&gt;%\n  filter(is.na(condition)==F) %&gt;%\n  ggplot(aes(x=blackdon, fill=condition))+\n  geom_histogram(alpha=.4)+\n  theme_bw()+\n  ggtitle(\"Distribution of Donations to Black Organizations\")+\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  facet_grid(~condition)+\n  xlab(\"Amount Donation (dollars)\")\n\n\n\n\n\nbanks %&gt;%\n  filter(is.na(condition)==F) %&gt;%\n  ggplot(aes(y=blackdon, x=condition, color=condition))+\n  geom_boxplot()+\n  geom_jitter(alpha=.5)+\n  theme_bw()+\n  ggtitle(\"Distribution of Donations to Black Organizations\")+\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  ylab(\"Amount Donation (dollars)\")\n\n\n\n\n\nbanks %&gt;%\n  filter(is.na(condition)==F) %&gt;%\n  ggplot(aes(x=blackdon, fill=condition))+\n  geom_density(alpha=.5)+\n  theme_bw()+\n  ggtitle(\"Distribution of Donations to Black Organizations\")+\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_grid(~condition)+\n  xlab(\"Amount Donation (dollars)\")\n\n\n\n\nHere is a way to show the means and confidence intervals for each condition. This figure is based on the plot in Figure 17.1 from Alex Coppock’s chapter in Advances in Experimental Political Science.\n\n## Find means and confidence intervals by condition\nbanks &lt;- subset(banks, is.na(condition)==F)\nm.cond &lt;- tapply(banks$blackdon, banks$condition, mean, na.rm=T)\nci.hope &lt;- t.test(banks$blackdon[banks$condition == \"Hope\"])$conf.int\nci.anger &lt;- t.test(banks$blackdon[banks$condition == \"Anger\"])$conf.int\nci.control &lt;- t.test(banks$blackdon[banks$condition == \"Control\"])$conf.int\ncombd &lt;- data.frame(cbind(cbind(m.cond),rbind(ci.control, ci.anger, ci.hope)))\nnames(combd) &lt;- c(\"Mean\", \"lower\", \"upper\")\ncombd$condition &lt;- c(\"Control\", \"Anger\", \"Hope\")\ncombd$condition &lt;- factor(combd$condition, levels=c(\"Control\", \"Anger\", \"Hope\"))\n\n\n## Note we draw from both data=combd and data=banks\nggplot(combd, aes(x=condition, y=Mean, color=condition)) +\n  geom_point(data = banks, aes(y=blackdon),\n    position = position_jitter(width = 0.2, height = 0.1),\n    alpha = 0.4) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) +\n  theme_bw() +\n  ggtitle(\"Donations to Black Organizations by Condition\")+\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(breaks = seq(0, 10, length.out = 5)) +\n  theme(axis.title.x = element_blank()) +\n  ylab(\"Donations (dollars)\")"
  },
  {
    "objectID": "05-Visualization.html#examples-of-arguments-in-plot",
    "href": "05-Visualization.html#examples-of-arguments-in-plot",
    "title": "5  Visualization",
    "section": "5.4 Examples of arguments in plot",
    "text": "5.4 Examples of arguments in plot\nHere are some common R plotting functions and arguments\nCreate a plot\n\nplot(): for scatterplots and trend plots\nbarplot(): for barplot comparisons across categories\nboxplot(): boxplot for summaries of numeric variables\nhist(): for histogram summaries of a single numeric variable\n\nAesthetic arguments within a plot\n\nmain =: Specifies the main title of the plot. Supply text (e.g., main = \"my title\")\nylab =: Specifies the title of the y-axis. Supply text (e.g., ylab = \"Mean of variable\")\nxlab =: Specifies the title of the x-axis. Supply text (e.g., xlab = \"X variable name\")\nylim =: Specifies the range of the y-axis. Supply vector of two numbers (e.g., ylim = c(0, 100))\nxlim =: Specifies the range of the x-axis. Supply vector of two numbers (e.g., xlim = c(0, 100))\nbty=\"n\": Removes the border box around the plot\ncex, cex.main, cex.names, cex.lab, cex.axis: Changes the size of different elements of a plot. Default is 1, so a value of .8 would be smaller than default, and 1.2 would be bigger than normal.\ntype =: Specifies the type of plot (e.g., type=\"l\" is a line plot, type=\"b\" is a plot with points and lines connecting them)\nlwd=: Specifies the width of a line on a plot. Default is 1. E.g., lwd=3 makes a line much thicker\npch=: Specifies the point type. E.g., pch=15\nlty=: Specifies the line type. E.g., lty=2 is a dashed line\ncol=: Specifies the color of the central element of the plot. Can take a single color or vector of colors. Use colors() in the console to see all R colors.\nnames: Specifies a set of labels in a barplot\n\nWays to annotate a plot (generally added below the initial plotting function)\n\nabline(): Adds a line to the plot at a particular point on the x- or y- intercept, either horizontal, vertical, or of a particular slope\n\nExample: Adding a horizontal line at a particular at a y value of 2 abline(h=2)\nExample: Adding a vertical line at a particular at a x value of 2 abline(v=2)\n\nlines(x=, y=): Adds a line connecting pairs of x- and y-coordinates. We used this to add the South line to the social mobility plot.\naxis(): Used to replace the default x- or y- axis that R will create with a customized axis\n\nTo create an original y-axis, use axis(2, vectorofvalues, labels) and specify yaxt=\"n\" inside the plotting function to remove the original y-axis.\nTo create an original x-axis, use axis(1, vectorofvalues, labels) and specify xaxt=\"n\" inside the plotting function to remove the original x-axis.\n\nlegend(): Adds a legend to a plot. Can specify the location as the first argument (e.g., \"bottomleft\" or \"topright\")\ntext(): Adds text to a plot at specific x- and y- locations. (E.g., text(x=3, y=4, \"Here is a point\"). The x and y arguments can be single numbers or a vector of numbers. x and y need to be the same length.\npoints(): Adds points to a plot at specific x- and y- locations. Inputs are much like plot"
  },
  {
    "objectID": "05-Visualization.html#creating-tables-from-r",
    "href": "05-Visualization.html#creating-tables-from-r",
    "title": "5  Visualization",
    "section": "5.5 Creating Tables from R",
    "text": "5.5 Creating Tables from R\nFormatting and Exporting R Results\nR has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output.\nHere is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word.\nYour choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word.\nA simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples:\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nfit &lt;- lm(call ~ race, data=mydata)\n\n\n## First time you use texreg, install it\ninstall.packages(\"texreg\")\n\nlibrary(texreg)\nhtmlreg(list(fit),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race\")\n\n\n\nRegression of Call Backs on Race\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nYou can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two:\nfit2 &lt;- lm(call ~ race + sex, data=mydata)\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\")\n\n\n\nRegression of Call Backs on Race and Sex\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.06***\n\n\n0.07***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nracewhite\n\n\n0.03***\n\n\n0.03***\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.01)\n\n\n\n\nsexmale\n\n\n \n\n\n-0.01\n\n\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n4870\n\n\n4870\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n5.5.1 Additional formatting examples\nHere are some additional examples with different formats. You can run them on your own computer to see what the output looks like.\nThe package texreg has three primary formats\n\ntexreg() for LATEX output;\nhtmlreg() for HTML, Markdown-compatible and Microsoft Word-compatible output;\nscreenreg() for text output to the R console.\n\nIf you are working with a LaTex document, I recommend using texreg(), which will output LaTex syntax in your R console, which you can copy and paste into your article document.\nNote: this function allows you to customize model and coefficient names.\n\nlibrary(texreg)\ntexreg(list(fit, fit2),\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are working with a Microsoft Word document, I recommend using htmlreg() and specifying a file name for your output. This will export a file to your working directory, which you can copy and paste into your Word article document. Otherwise, the syntax is the same as above.\n\nlibrary(texreg)\nhtmlreg(list(fit, fit2), file = \"models.doc\",\n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nIf you are trying to read the output in your R console, that’s when I would use screenreg(). However, for professional manuscript submissions, I would recommend the other formats.\n\nlibrary(texreg)\nscreenreg(list(fit, fit2), \n        stars=c(0.001, 0.01, 0.05),\n        caption = \"Regression of Call Backs on Race and Sex\",\n       custom.model.names = c(\"Bivariate\", \"Includes Sex\"),\n       custom.coef.names = c(\"Intercept\",\n                             \"Race- White\",\n                             \"Sex- Male\"))\n\nThe package stargazer allows similar options. I don’t think there are particular advantages to either package. Whatever comes easiest to you. The default for stargazer will output LaTex code into your R console.\n\nNote that the syntax is similar but has slightly different argument names from the texreg package.\nAlso, the intercept is at the bottom by default for stargazer. Be careful of the covariate ordering when you add labels.\n\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), \n        star.cutoffs=c(0.05,0.01, 0.001),\n        title= \"Regression of Call Backs on Race and Sex\",\n        dep.var.labels.include = F,\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\nYou can adjust the type of output in stargazer for other formats, similar to texreg. Here is an example of Microsoft Word output.\n\nlibrary(stargazer)\nstargazer(list(fit, fit2), out = \"modelstar.doc\", type=\"html\",\n        star.cutoffs=c(0.05,0.01, 0.001),\n        dep.var.labels.include = F,\n        title= \"Regression of Call Backs on Race and Sex\",\n       column.labels = c(\"Call Back\", \"Call Back\"),\n       covariate.labels = c(\"Race- White\",\n                             \"Sex- Male\",\n                             \"Intercept\"))\n\n\n\n5.5.2 Additional Table Types\nSometimes you might want to create tables that are not from regression models, such as tables for descriptive statistics. R has other packages for tables of this type.\nFor example xtable can create simple html and latex tables. You just have to supply the function with a table object or matrix.\nHere is a first example making a formated table using crosstabs of two variables.\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nlibrary(xtable)\ntable1 &lt;- table(race = mydata$race, sex = mydata$sex)\n\n## RMarkdown html\nprint(xtable(table1), type=\"html\")\n\n\n\n\n\n\n\n\n\nfemale\n\n\nmale\n\n\n\n\nblack\n\n\n1886\n\n\n549\n\n\n\n\nwhite\n\n\n1860\n\n\n575\n\n\n\n\n\n## LaTeX\nprint(xtable(table1))\n\n\n## Word\nprint(xtable(table1), type=\"html\", file = \"crosstab.doc\")\n\nExample with a t-test\n\nWe assemble the results in a vector. Using rbind makes it into a matrix object.\nIf you had multiple t-tests, you could rbind() several vector rows together into one xtable\nI extracted the estimates, t-statistic, and confidence intervals. You could also extract the p-value.\n\n## Run t-test and gather results\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv\")\nt.resume &lt;- t.test(mydata$call[mydata$race == \"black\"],\n                   mydata$call[mydata$race == \"white\"])\n\nestimates &lt;- t.resume$estimate\ncinterval &lt;- t.resume$conf.int\ntstat &lt;- t.resume$statistic\n\nresults &lt;- c(estimates, cinterval, tstat)\nnames(results) &lt;- c(\"Mean Black App\", \"Mean White App\", \"Lower CI\", \"Upper CI\", \"t-statistic\")\nresults &lt;- rbind(results)\nlibrary(xtable)\n## Rmarkdown html\nprint(xtable(results), type=\"html\", include.rownames = F)\n\n\n\n\n\n\n\nMean Black App\n\n\nMean White App\n\n\nLower CI\n\n\nUpper CI\n\n\nt-statistic\n\n\n\n\n0.06\n\n\n0.10\n\n\n-0.05\n\n\n-0.02\n\n\n-4.11\n\n\n\n\nlibrary(xtable)\n## Word doc\nprint(xtable(results), type=\"html\", include.rownames = F, file=\"tresults.doc\")\nlibrary(xtable)\n## Latex\nprint(xtable(results), include.rownames = F)"
  },
  {
    "objectID": "06-ethics.html#value-of-informed-consent",
    "href": "06-ethics.html#value-of-informed-consent",
    "title": "6  Ethics and Sampling Considerations",
    "section": "6.1 Value of Informed Consent",
    "text": "6.1 Value of Informed Consent\nBelow we will use some of these questions to guide our discussion.\n\nAs Teele reviews, the Belmont report covers three principles. How should we define these?:\n\nBeneficence\nRespect for persons\nJustice\n\nHere is a summary of different forms of consent:\n\nWhich one is the most common for the experiments we have designed?\nWhat is the value of informed consent? What does it try to achieve\n\nYour ideas …\n\nWhen might informed consent undermine research goals? Can it ever actually increase harm to subjects?\n\nYour ideas …\n\nHow can we resolve tradeoffs between informed consent and measurement? Should we? Must we?\n\nYour ideas …\n\nIn cases where we do not get informed consent for seemingly valuable reasons, can this go awry? Are there potential downstream consequences? Does it depend on the sample size? Or study design? Is the research still worth it in the end?\n\nYour ideas …\n\nSee example mentioned in response to a study varying the names used in emails to Colorado county clerks:\n“My name is Karim and I hope you are well. I found your contact information in a voting resources directory and I want to ask about the voting process. What do I need to bring to vote? I want to vote for president but I did not register with a political party. Do I have to do that before I vote. And if I have to work late will I still be able to vote in time.”\nSee example of mailers sent to more than 100,000 Montana registered voters for a nonpartisan judicial election."
  },
  {
    "objectID": "06-ethics.html#research-integrity-reproducibility-and-transparency",
    "href": "06-ethics.html#research-integrity-reproducibility-and-transparency",
    "title": "6  Ethics and Sampling Considerations",
    "section": "6.2 Research Integrity, Reproducibility, and Transparency",
    "text": "6.2 Research Integrity, Reproducibility, and Transparency\n\nHow do the incentives that structure academia encourage vs. discourage research fraud?\n\nYour ideas …\n\nAre there steps the field has taken / can take to detect and mitigate it?\n\nYour ideas …\n\n\nWhen our findings don’t replicate, how should we interpret this? Does it mean the original result was a false positive?\n\nYour ideas …\n\n\n\n6.2.1 Preregistration\nWhat is pre-registration? (from Boudreau)\n\n“Practice of developing one’s research questions, hypotheses, research design, and analyses before observing the data and making that information public on an independent registry.”\n“Researchers may also create and submit pre-analysis plans that describe in detail the procedures they will use when collecting and analyzing the data (e.g., planned data analyses and statistical tests).”\nThese can also include standard operating procedures\nHere is a short guide to a pre-analysis plan from EGAP\nExamples of pre-registration registries: Aspredicted.org; Open Science Framework; EGAP (now hosted by OSF)\nNote that some journals now require pre-registration for experiments. E.g., The Journal of Politics\nSome journals now offer a chance to submit a registered report, where your paper is reviewed blind to the results. E.g., Journal of Experimental Political Science. See a discussion from the editor Vin Arceneaux here.\n\n\nWhat items should be included in a pre-registration plan?\n\nYour ideas …\n\nWhat are the benefits of pre-registration? Are there downsides?\n\nYour ideas …\n\n\n\n6.2.2 Reporting an Experimental Analysis\n(from Boudreau and Gerber et al. 2015)\n\nEligibility and exclusion criteria for participants\nDetails of recruitment and selection of participants, including incentives and any firms used\nType of experiment (lab, survey, field), mode, location, and dates conducted\nResponse rate or other participation metric (and how calculated), when possible\nDetails of randomization procedure\nBaseline means and standard deviations for demographics and other pretreatment measures by experimental group\nWhether blinding took place and how it was accomplished\nDescription of the treatment(s), as well as description of the control group\nDetails of experiment: its duration, number of participants, within- versus between-subject design, piggybacking/ordering/repetition of treatments, use of deception, use of incentives\nEvidence treatment was delivered as intended, if available\nDefinitions of outcome measures and covariates, as well as noting whether the level of analysis differs from the level of randomization\nIdentification of analyses specified ex ante versus ex post exploratory analyses\nInformation in CONSORT participant flow diagram\nSample means and standard deviations for outcome variables using intent-to-treat analysis\nPatterns of missing data, attrition, and methods of addressing these issues if missing data and/or attrition are present\nDescription of weighting procedures, if used\nInstitutional review board approval, preregistration, source of funding, conflicts of interest\nAvailability of replication materials and data set\n\nMany researchers share data via Dataverse, OSF, or Github"
  },
  {
    "objectID": "06-ethics.html#failed-designs",
    "href": "06-ethics.html#failed-designs",
    "title": "6  Ethics and Sampling Considerations",
    "section": "6.3 “Failed Designs”",
    "text": "6.3 “Failed Designs”\nWhat can we do in our experimental designs to allow learning even when we do not get significant results?\n\nYour ideas …\n\nKane (2024) offers a few ideas. Table 1 provides tips on approaching these seven alternative explanations.\n\nCheck for inattentiveness. How can we do this?\nDid the treatment vary the independent variable of interest?\nWere respondents pre-treated– maybe even outside the context of the study?\nIs there enough statistical power?\nDo we have a sound measure of our outcome? In what way? How can this go wrong?\nDo we have ceiling/floor effects?\nAre there counterveiling treatment effects? How can we look for these?"
  },
  {
    "objectID": "06-ethics.html#sampling-considerations",
    "href": "06-ethics.html#sampling-considerations",
    "title": "6  Ethics and Sampling Considerations",
    "section": "6.4 Sampling Considerations",
    "text": "6.4 Sampling Considerations\n\nWhat makes a sample a good sample?\n\nYour ideas …\n\nWho is in our sample? For an average person’s discussion of polling and sampling and participating in surveys, see 37-39:40 minutes of the Nateland podcast.\nHow can we check for data quality? What elements are a part of data quality? I.e., what should we be worried about?\n\nYour ideas …\n\nWhat are examples of bot checks/attention checks?\n\nYour ideas …\n\nWhen should we actually exclude subjects? When should we not?\n\nYour ideas …\n\n\n6.4.1 Power Analysis\nWe are often concerned about guarding against false positives. We do this by setting a conservative threshold for judging significance in hypothesis testing.\n\nType I error: “false positive”: the error of rejecting a null hypothesis when it is actually true +Conventionally, our tolerance for false positives are \\(\\alpha = 0.05\\).\nType II error: “false negative”: conclude there is no effect (failing to reject the null) when there is one.\n\nWe tend to refer to this as \\(\\beta\\) and statistical power is \\(1-\\beta\\) (true positive)\n\n\nWhat is a test’s Power?\n\nPower helps us guard against false negatives. It is the probability of a true positive:\n\nFinding a significant effect if one is there,\n(1- Type II) where a Type II error is when you conclude there is no effect when there is one.\n\nSee discussion on power from EGAP.\n\n\\[\\begin{align*}\n1 - Pr(\\text{Type II error}) &= 1 - \\beta\\\\\n&=  \\underbrace{\\Phi (\\frac{| \\mu_t -\\mu_c|\\sqrt{N}}{2\\sigma} - \\Phi^{-1}(1 - \\frac{\\alpha}{2}))}_{\\text{A common formula}} \\\\\n&=  \\Phi (\\frac{| \\mu_t -\\mu_c|\\sqrt{N}}{2\\sigma} - \\underbrace{ 1.96}_{\\text{At conventional levels}})\\\\\n&= \\text{Prob test stat exceeds threshold for rejecting null}\n\\end{align*}\\]\nTerms\n\n\\(\\beta\\) is measure of power, between 0 and 1.\n\\(\\Phi\\) is the CDF of the normal distribution (think: area under the curve), and \\(\\Phi^{-1}\\) is its inverse.\n\\(\\mu_t - \\mu_c\\) is the difference in average outcomes in the treatment and control groups.\n\\(\\sigma\\) is the standard deviation of outcomes.\n\\(\\alpha\\) is our significance level - conventionally, 0.05.\n\\(N\\) is the total number of subjects. This is the only variable that is under the direct control of the researcher.\n\nHelpful video\nRecall that t-statistics beyond the critical values (e.g., 1.96) will result in rejecting the null hypothesis.\n\n\n\n\n\nWe want to know the probability that our test statistic will fall in this rejection region.\n\n\n\n\n\n\n\n6.4.2 Power in R\nTo conduct a power analysis\nWe need all but one of:\n\nsample size\neffect size in population\nstandard deviation of outcome in population\ndesired power level\nsignificance level\n\nWhat makes this calculation difficult?\nFor continuous variables, we can calculate the power of either one-sample or two-sample test using the command power.t.test(n, delta, sd, sig.level, power, type, alternative).\n\nn is the number of observations;\ndelta is the true difference in means;\nsd is the standard deviation within the population;\nsig.level is the test’s level of significance (Type I error probability);\ntype is the type of t-test (“two.sample”, “one.sample” or “paired”);\nalternative specifies a direction of the test (“two.sided” or “one.sided”)\npower is the power of the test\n\nNote on effect sizes\nCohen’s \\(d = \\frac{delta}{\\sigma}\\) = \\(\\frac{\\tt delta}{\\tt sd}\\)\n\nProblem: We usually don’t know \\(\\sigma\\) or delta.\n\nSolution 1: Use sample data for pooled standard deviation (\\(\\hat{s}_y\\)) and difference in means (\\(\\bar{y}_T - \\bar{y}_C\\)).\nSolution 2: Use rules of thumb, .2, .5, .8 (e.g., delta = .5 and d = 1})\n\nCohen,Jacob.1992.Statistical power analysis.Psychological Science\n\n\n## Leave one argument blank or = NULL\n\n## Power for an 800-person study with .25 effect size and 400-person groups\npower.t.test(n= 400,\n             delta = .25, sd=1, sig.level = .05,\n             power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 400\n          delta = 0.25\n             sd = 1\n      sig.level = 0.05\n          power = 0.9419449\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n## What effect size would we need for 80% power?\npower.t.test(n= 400,\n             delta = NULL, sd=1, sig.level = .05,\n             power = .8)\n\n\n     Two-sample t test power calculation \n\n              n = 400\n          delta = 0.1983417\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n6.4.2.1 Additional Resources for Power in R\nPower analysis in Conjoint Experiments by Martin Lukac: tool\nR resource: Additional functions from Statmethods\nFor proportions, The command power.prop.test(n, p1, p2, sig.level, power, alternative) may be used to calculate the power. Note that this command may only be used to calculate power for a two-sample test.\n\nn is the number of observations per group (assumes equal size);\np1 the proportion in group 1;\np2 the proportion in group 2;\nsig.level is the test’s level of significance\nalternative specifies a direction of the test (“two.sided” or “one.sided”);\npower specifies power of the test\n\n\n## What sample size for difference in proportions at 80% power?\npower.prop.test(n=NULL, p1 = .75, p2=.80,\n                sig.level=.10, power = .8)\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 861.4198\n             p1 = 0.75\n             p2 = 0.8\n      sig.level = 0.1\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n6.4.3 Relationship between Error Rates and Multiple Testing\n\\[\\begin{align*}\nPr(\\text{at least one significant result}) &= 1 - Pr(\\text{no significant result})\\\\\n&= 1 - (1 - 0.05)^{\\text{number of tests}}\n\\end{align*}\\]\nWith 20 tests, you have a 64% chance of observing at least one significant result even if all are not significant.\n\n1 - (1 - 0.05)^20\n\n[1] 0.6415141\n\n\nFor this reason, researchers may make adjustments to p-values when they have several tests in a single analysis. See EGAP’s resource"
  },
  {
    "objectID": "07-FieldExperiments.html#field-experiment-application",
    "href": "07-FieldExperiments.html#field-experiment-application",
    "title": "7  Field Experiments",
    "section": "7.1 Field Experiment Application",
    "text": "7.1 Field Experiment Application\nKarpowitz, Monson, and Preece (2017), “How to Elect More Women: Gender and Candidate Success in a Field Experiment.”\n\nprop_sd_fem2014: proportion of state delegates who are women\ncondition: the treatment condition in which precincts were assigned: Control, Supply, Demand, or Supply + Demand\ncounty the county in which precincts were located\n\nWhat was their research question? What is \\(Y_i(1)\\)? What is \\(Y_i(0)\\)?\n\nWhy is this considered a field experiment?\n\nLet’s load the data and replicate the results in Table 3, column 1 of the paper.\n\nlibrary(foreign)\nwom &lt;- read.dta(\"data/karpetal.dta\")\n\nLet’s look at the treatment conditions\n\n## Treatment indicator\ntable(wom$condition)\n\n\n      Control        Supply        Demand Supply+Demand \n          453           470           446           443 \n\nclass(wom$condition)\n\n[1] \"factor\"\n\n## Dependent variable\nsummary(wom$prop_sd_fem2014)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   0.263   0.500   1.000 \n\n\nWe can now estimate the treatment effects. Note: We want to compare each treatment condition to the control condition. How should we do this?\n\nWe could conduct individual t.test functions for each pairwise comparison.\nAlternatively, if we use regression, we can interpret each coefficient estimate as that difference in means between the treatment condition and the category left out of the regression.\n\n\n## Regression approach\nr1 &lt;- lm(prop_sd_fem2014 ~ condition, wom)\nround(summary(r1)$coefficients, digits=4)\n\n                       Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.2457     0.0167 14.7226   0.0000\nconditionSupply          0.0149     0.0234  0.6381   0.5235\nconditionDemand          0.0150     0.0237  0.6335   0.5265\nconditionSupply+Demand   0.0396     0.0237  1.6692   0.0953\n\n\nNote how the Control category is left out. We can interpret each coefficient estimate as the difference in the proportion of delegates who are female between the Control condition and corresponding other condition. In this regression, we can also interpret the Intercept as the value our dependent variable takes when all of the other variables are 0 (i.e., when we are in the control condition). Before we interpret the significance of these effects, let’s take a closer look at the design."
  },
  {
    "objectID": "07-FieldExperiments.html#clustering-standard-errors",
    "href": "07-FieldExperiments.html#clustering-standard-errors",
    "title": "7  Field Experiments",
    "section": "7.2 Clustering standard errors",
    "text": "7.2 Clustering standard errors\nOften, we draw a sample of independent observations where randomization occurs at the level of the individual unit. But, sometimes we have multiple observations per unit (e.g., multiple observations of a particular individual, multiple individuals in a household)\n\nExample: In Karpowitz et al., precincts are nested within counties. As footnote 15 notes, “The state party relies heavily on county-level party officials to organize and run the neighborhood caucus meetings.”\nConsequence: our observations are no longer independent.\nSolution: we must account for this in estimates of uncertainty.\n\nWe are going to “cluster” our standard errors by county\n\n\nIt used to be pretty difficult to do this in R, but over the last few years, people have developed packages to integrate clustering into the standard functions for regression. We will use the package estimatr in this way.\n\ninstall.packages(\"estimatr\")\n\nIts primary function is lm_robust reflecting its easy integration of different types of “robust” standard errors.\n\nlibrary(estimatr)\nr1.cluster &lt;- lm_robust(prop_sd_fem2014 ~ condition, wom,\n                        se_type=\"stata\", clusters = county)\nround(summary(r1.cluster)$coefficients, digits=4)\n\n                       Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper\n(Intercept)              0.2457     0.0146 16.8311   0.0000   0.2158   0.2756\nconditionSupply          0.0149     0.0155  0.9639   0.3433  -0.0168   0.0466\nconditionDemand          0.0150     0.0214  0.7012   0.4890  -0.0288   0.0589\nconditionSupply+Demand   0.0396     0.0160  2.4805   0.0194   0.0069   0.0723\n                       DF\n(Intercept)            28\nconditionSupply        28\nconditionDemand        28\nconditionSupply+Demand 28\n\n\nNote how the coefficient estimates remain unchanged, but the size of the standard errors, and therefore, the corresponding t-values and p-values also change.\n\n7.2.1 Adjusting p-values\nAnother thing we might note about this test is that we are conducting three hypothesis tests simultaneously. This might bring up issue related to multiple comparisons problems. Some of the figures that the authors present include a “Bonferroni” adjustment to the conventional significance levels. When we have multiple hypothesis tests, this can rapidly increase the possibility that we observe at least one significant result, particularly if we assume the tests are independent:\n\\[\\begin{align*}\nPr(\\text{at least one significant result}) &= 1 - Pr(\\text{no significant result})\\\\\n&= 1 - (1 - 0.05)^{\\text{number of tests}}\n\\end{align*}\\]\nWith 3 tests, you have a 14% chance of observing at least one significant result even if all are not significant.\n\n1 - (1 - 0.05)^3\n\n[1] 0.142625\n\n\nWe can make an adjustment that makes it harder for us to conclude a result is significant under multiple comparisons. See EGAP’s resource.\nHere are some examples of adjustments using the p.adjust function.\n\n## extract p-values from the regression\npvals &lt;- summary(r1.cluster)$coefficients[2:4, 4]\npvals\n\n       conditionSupply        conditionDemand conditionSupply+Demand \n            0.34334883             0.48898188             0.01939968 \n\n## bonferroni-- very conservative\np.adjust(pvals, method=\"bonferroni\")\n\n       conditionSupply        conditionDemand conditionSupply+Demand \n            1.00000000             1.00000000             0.05819905 \n\n\n\n\nThe manual approach.\n\n\n## manual (p-vals over 1 switch to 1)\nm &lt;- 3 # number of tests\npvals * 3\n\n       conditionSupply        conditionDemand conditionSupply+Demand \n            1.03004649             1.46694564             0.05819905 \n\n\n\n\n## holm\np.adjust(pvals, method=\"holm\")\n\n       conditionSupply        conditionDemand conditionSupply+Demand \n            0.68669766             0.68669766             0.05819905 \n\n\n\n\nThe manual approach.\n\n\n## manual-- requires pval sorting\nm &lt;- 3 # number of tests\ni &lt;- 1:3 # sequence of rankings\ncummax((m + 1 - i) * sort(pvals))\n\nconditionSupply+Demand        conditionSupply        conditionDemand \n            0.05819905             0.68669766             0.68669766 \n\n\n\nAs you read papers, take note of whether authors make these types of adjustments. There are different philosophies about when one must account for multiple comparisons.\n\n\n7.2.2 Additional complex sampling designs\nClustered random assignment\nSometimes we go so far as to assign treatment by cluster (e.g., by household, village, school)\n\nConsequence: our observations are no longer independent.\nSolution: we must account for this in estimates of uncertainty.\n\nCaution: if cluster size is correlated with potential outcomes, can bias ATE. See Gerber and Green (2012, 83).\nCaution: clustered random assignment can increase uncertainty by decreasing the degrees of freedom (you use this when calculating the p-value from the t distribution)\n\n\nWhy cluster if clustering is not ideal?\nBlocking\nSometimes we assign randomization within specific subgroups instead of at the individual level (Example: Karpowitz et al. replication experiment, pg. 12, where they randomize treatment in their survey experiment by respondent gender and by whether the respondent was from the state above or below average in female representation among Republican state legislators.)\n\nEnsures balance across conditions within (blocked) subgroups\nRelatedly, avoids need for additional covariate adjustment\nFacilitates subgroup analysis by suggesting that is is particularly important to have balance within these subgroups\nSometimes improves precision of estimates\nCaution:\n\nAffects how to calculate ATEs if assignment probabilities differ by block) See Gerber and Green Chapter 4.5.\nAffects calculation of SEs!! See Gerber and Green pgs. 73-74\n\n\nMatched Pairs\nMatched pairs is a special case of blocking. We find two units OR two clusters of units that are most closely “matched” on pre-treatment covariates\n\nCan use matching algorithms to do this\nAssign treatment randomly at the pair level, i.e., coin flip as to which unit in the pair receives treatment\nCaution: Must take this design into account in analysis. See Gerber and Green (2012, 77)\n\nFor an experimental application with matched pairs, see “Empowering Women through Development Aid: Evidence from a Field Experiment in Afghanistan” by Andrew Beath, Christia Fotini, and Ruben Enikolopov publised in The American Political Science Review in 2013. The screenshot below describes the matched pairs process. In the caption of their table of results, they describe how they accounted for matched pairs in the regression."
  },
  {
    "objectID": "07-FieldExperiments.html#compliance",
    "href": "07-FieldExperiments.html#compliance",
    "title": "7  Field Experiments",
    "section": "7.3 Compliance",
    "text": "7.3 Compliance\nAn issue with all experimental designs, but particularly field experiments, is compliance, which refers to whether respondents received the treatment as assigned.\nDo subjects assigned to treatment receive treatment?\n\nEncouragement design: Treatment assignment might only “encourage” receipt of treatment, is an intent-to-treat\n\nNote how Karpowitz et al. label Table 3 as “Intent-to-Treat” effects\n\nCompliance refers to whether receipt of treatment aligns with assignment to treatment\n\nOne-sided non-compliance: some individuals assigned to treatment don’t receive treatment failure-to-treat\nTwo-sided non-compliance: possibility of both failure-to-treat and that those not assigned to treatment, receive treatment\n\n\nIn the Karpowitz et al. article, what might cause issues with compliance?\n\n7.3.1 Notation for Compliance\nLet \\(d_i(z)\\) be the actual treatment status of unit i where \\(z\\) is the experimental assignment.\n\nPractice: what does \\(d_i(1) = 0\\) mean, in words?\n\nWhen everyone assigned to treatment receives the treatment \\(d_i = z_i\\).\nWe can break subjects into four types based on their compliance: \\[\\begin{align*}\n\\mbox{Compliers: } &d_i(1)=1; d_i(0)=0 \\\\\n&\\implies Y_i(d_i(1)) - Y_i(d_i(0)) = Y_i(1) - Y_i(0) \\\\\n\\mbox{Always takers: } &d_i(1)=1; d_i(0)=1 \\\\\n&\\implies Y_i(d_i(1)) - Y_i(d_i(0)) = Y_i(1) - Y_i(1)  \\\\\n\\mbox{Never takers: } &d_i(1)=0; d_i(0)=0 \\\\\n&\\implies Y_i(d_i(1)) - Y_i(d_i(0)) = Y_i(0) - Y_i(0) \\\\  \n\\mbox{Defiers: } &d_i(1)=0; d_i(0)=1 \\\\\n&\\implies Y_i(d_i(1)) - Y_i(d_i(0)) = Y_i(0) - Y_i(1)\n\\end{align*}\\]\nWhat does \\(ATE|d_i(1) &gt; d_i(0)\\) mean, in words?\nWith full compliance ATE = ITT. With non-compliance, our standard estimation of the treatment effect is now the ITT.\n\nWhere we estimate \\(ITT_i = Y_i(z = 1) - Y_i(z=0)\\)\n\nExample: Karpowitz et al. stick with the ITT\n\nWith assumptions, we may be able to identify and estimate the Complier Average Causal Effect, the average treatment effect just among compliers.\n\n\n\n7.3.2 Complier Average Causal Effect\n\\(CACE = E[Y_i(d = 1) - Y_i(d = 0) | d_i(1) &gt; d_i(0)]\\)\nWhile it may be incredibly tempting, to estimate this by just subsetting on receipt of treatment instead of experimental assignment, we cannot simply subset our treatment group to only include those who received the treatment. Why not?\nCACE Additional Identification Assumptions\n\nExclusion restriction: \\(Y_i(z, d) = Y_i(d)\\) where z is experimental assignment\nMonotonicity (i.e., no defiers): \\(d_i(1) \\geq d_i(0)\\) for all \\(i\\)\nIn practice, to actually estimate the effect, you need data–and accurate data– on receipt of treatment. (Not always possible.)\n\nHow would one identify compliance in the Karpowitz et al. article?\nHow would one identify compliance in the Siegel and Badaan article?\n\n\nEstimation Process is in two stages\n\n\\(\\widehat{ITT}_D\\): Estimate the effect of experimental assignment on receipt of treatment\n\nThis is the proportion of compliers when assuming monotonicity (no defiers). Note: This does not necessarily identify who is a complier, just the proportion of compliers.\n\n\\(\\widehat{ITT}\\): Estimate the effect of experimental assignment on the outcome\nScale the ITT by receipt of treatment: \\(\\widehat{CACE} = \\frac{\\widehat{ITT}}{\\widehat{ITT}_D}\\)\n\nWe have to be in a world where \\(ITT_D &gt; 0\\) (at least one complier) to identify the CACE\n\n\nWe can take the example from Gerber and Green 2012.\n\nWhat is the effect of canvassing on voter turnout?\n\nThe experiment assigned some people to be canvassed, some not to be canvassed.\nThe treatment is actually having contact with the canvasser.\nWhat could be a compliance issue here?\n\n\nTwo-stages = two regressions\n\nOur outcome is whether someone \\(Y=\\) voted.\nWe have an experimental \\(z=\\) assignment variable\nAnd we have a compliance variable \\(d=\\) treated\n\n\n## Replicating Chapter 5 analyses\n## z = Assigned, d = Treated, y= Voted\nlibrary(foreign)\nvoters &lt;- read.dta(\"data/ggch5.dta\")\n\n## 1. effect of experimental assignment on receipt of treatment\nITTd &lt;- lm(treated ~ assigned, voters)\n\n## 2. effect of experimental assignment on whether voted (outcome)\nITT &lt;- lm(voted ~ assigned, voters)\n\n## 3. ratio of treatment effect. compare with Box 5.6\nITT$coefficients[2]/ITTd$coefficients[2]\n\n assigned \n0.1407115 \n\n\nAlternative, recommended approach using ivreg. This will calculate the correct standard errors for this type of setup using instrumental variables regression. The experimental assignment is considered an “instrument” for the treatment.\n\ninstall.packages(\"AER\")\n\n\nlibrary(AER) \n## Function for conducting two-stage least squares regression\n## Takes form ivreg(Y ~ d | z, data)\ncace &lt;- ivreg(voted ~ treated  | assigned, data = voters)\nsummary(cace)$coefficients[2,]\n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.140711507 0.052276906 2.691657117 0.007126494 \n\n## Optional, more conservative standard errors\ncoeftest(cace, vcovHC(cace))[2,] \n\n   Estimate  Std. Error     t value    Pr(&gt;|t|) \n0.140711507 0.052433888 2.683598551 0.007300369 \n\n\nSubstantively, when might be interested in the CACE? When might we not?\n\nCaution: is compliance data accurate? is compliance rate low or high?\n\n\n\n7.3.3 Design-based approach to help measure compliance\nUse a placebo treatment (e.g., Broockman and Kalla), which may allow you to observe compliance in both the treatment and control conditions.\n\nNote that Broockman and Kalla report Complier Average Causal Effects comparing those in the assigned treatment and placebo conditions.\n\nIn their supplemental materials, they note how this is calculated: “501 voters identified themselves at the door after the initial greeting that did not differ by condition.”\n\nNote this gives them justification for subsetting and comparing these individuals\n\n“Canvassers then either began an intervention conversation or a placebo conversation. Of the 246 voters who identified themselves at their doors in the treatment group, 192 began the conversation and at least described their initial view on the law to the canvasser, rather than refusing to talk at all after identifying themselves. On the other hand, the treatment was inadvertendly delivered to 11 individuals in the placebo group due to canvasser error.”\n\n“Consistent with our pre-analysis plan, we report estimated complier average causal effects for the intervention under the assumptions that 1) there was no effect of the intervention for the voters who immediately refused to talk, and 2) there are no defiers; that is, no voters only received the intervention if they were assigned to the placebo group yet would not have received it were they actually in the treatment group.”"
  }
]